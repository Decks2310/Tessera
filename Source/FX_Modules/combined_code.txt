================================================================================
// File: AdvancedCompressorProcessor.cpp
================================================================================

// File: FX_Modules/AdvancedCompressorProcessor.cpp
#include "AdvancedCompressorProcessor.h"

AdvancedCompressorProcessor::AdvancedCompressorProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_ADVCOMP_";
    topologyParamId = slotPrefix + "TOPOLOGY";
    detectorParamId = slotPrefix + "DETECTOR";
    thresholdParamId = slotPrefix + "THRESHOLD";
    ratioParamId = slotPrefix + "RATIO";
    attackParamId = slotPrefix + "ATTACK";
    releaseParamId = slotPrefix + "RELEASE";
    makeupParamId = slotPrefix + "MAKEUP";
}

void AdvancedCompressorProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    currentSampleRate = sampleRate;
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };

    peakDetector.prepare(spec);
    peakDetector.setAttackTime(0.1f); // Fast ballistics for the detector itself
    peakDetector.setReleaseTime(5.0f);

    envelopeSmoother.prepare(spec);
    makeupGain.prepare(spec);
    colorationStage.prepare(spec);

    // Initialize RMS averages
    rmsAverages.resize(spec.numChannels, 0.0f);
    // Calculate alpha for RMS moving average (Blueprint 3.2.1)
    if (sampleRate > 0)
        rmsAlpha = std::exp(-1.0f / (float)(sampleRate * rmsWindowTimeMs / 1000.0f));

    reset();
}

void AdvancedCompressorProcessor::reset()
{
    peakDetector.reset();
    envelopeSmoother.reset();
    makeupGain.reset();
    std::fill(rmsAverages.begin(), rmsAverages.end(), 0.0f);
}

void AdvancedCompressorProcessor::releaseResources() {}

// RMS calculation using Exponential Moving Average (EMA)
float AdvancedCompressorProcessor::calculateRMS(int channel, float input)
{
    if (channel >= (int)rmsAverages.size()) return 0.0f;

    float squaredInput = input * input;
    // EMA: avg = alpha * avg + (1 - alpha) * input^2
    rmsAverages[channel] = rmsAlpha * rmsAverages[channel] + (1.0f - rmsAlpha) * squaredInput;
    return std::sqrt(rmsAverages[channel]);
}

// Gain calculation (Hard Knee implementation)
float AdvancedCompressorProcessor::calculateGainDb(float detectorDb, float thresholdDb, float ratio)
{
    if (detectorDb > thresholdDb)
    {
        return (thresholdDb - detectorDb) * (1.0f - (1.0f / ratio));
    }
    return 0.0f;
}

// Configures characteristics based on the selected topology (Blueprint 3.3)
void AdvancedCompressorProcessor::configureTopology(Topology topology, float attackMs, float releaseMs)
{
    switch (topology)
    {
    case Topology::VCA_Clean:
        // VCA: Fast, clean (Blueprint 3.3.1)
        envelopeSmoother.setAttackTime(attackMs);
        envelopeSmoother.setReleaseTime(releaseMs);
        colorationStage.functionToUse = [](float x) { return x; }; // Transparent
        break;
    case Topology::FET_Aggressive:
        // FET: Ultra-fast attack, aggressive coloration (Blueprint 3.3.2)
        envelopeSmoother.setAttackTime(juce::jmax(0.1f, attackMs * 0.5f)); // Faster attack
        envelopeSmoother.setReleaseTime(releaseMs);
        // FET coloration profile approximation
        colorationStage.functionToUse = [](float x) { return std::tanh(x * 1.5f); };
        break;
    case Topology::Opto_Smooth:
        // Opto: Slower attack, smoother release (Blueprint 3.3.3)
        envelopeSmoother.setAttackTime(juce::jmax(10.0f, attackMs * 1.5f)); // Inherently slower attack
        envelopeSmoother.setReleaseTime(releaseMs * 1.2f);
        // Opto/Tube coloration approximation
        colorationStage.functionToUse = [](float x) { return std::tanh(x * 0.8f); };
        break;
    }
}

void AdvancedCompressorProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&)
{
    juce::ScopedNoDenormals noDenormals;
    // P1 Boilerplate
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());

    // 1. Get parameters (Safety check omitted for brevity)
    // Ensure parameters exist before accessing them
    if (!mainApvts.getRawParameterValue(thresholdParamId) || !mainApvts.getRawParameterValue(ratioParamId) ||
        !mainApvts.getRawParameterValue(attackParamId) || !mainApvts.getRawParameterValue(releaseParamId) ||
        !mainApvts.getRawParameterValue(makeupParamId) || !mainApvts.getRawParameterValue(topologyParamId) ||
        !mainApvts.getRawParameterValue(detectorParamId))
    {
        return;
    }

    float thresholdDb = mainApvts.getRawParameterValue(thresholdParamId)->load();
    float ratio = mainApvts.getRawParameterValue(ratioParamId)->load();
    float attackMs = mainApvts.getRawParameterValue(attackParamId)->load();
    float releaseMs = mainApvts.getRawParameterValue(releaseParamId)->load();
    float makeupDb = mainApvts.getRawParameterValue(makeupParamId)->load();
    auto topology = static_cast<Topology>(static_cast<int>(mainApvts.getRawParameterValue(topologyParamId)->load()));
    auto detectorMode = static_cast<DetectorMode>(static_cast<int>(mainApvts.getRawParameterValue(detectorParamId)->load()));

    // 2. Configure based on topology
    configureTopology(topology, attackMs, releaseMs);
    makeupGain.setGainDecibels(makeupDb);

    int numSamples = buffer.getNumSamples();
    int numChannels = buffer.getNumChannels();

    // Process loop: Sample outer, Channel inner
    for (int i = 0; i < numSamples; ++i)
    {
        // Process channels independently
        for (int ch = 0; ch < numChannels; ++ch)
        {
            float inputSample = buffer.getSample(ch, i);

            // --- DETECTOR STAGE (Blueprint 3.2.1) ---
            float detectorValue = 0.0f;
            if (detectorMode == DetectorMode::Peak)
            {
                // BallisticsFilter requires the channel index (ch)
                detectorValue = peakDetector.processSample(ch, std::abs(inputSample));
            }
            else // RMS
            {
                detectorValue = calculateRMS(ch, inputSample);
            }

            float detectorDb = juce::Decibels::gainToDecibels(detectorValue + 1e-9f);

            // --- GAIN COMPUTER ---
            float targetGainDb = calculateGainDb(detectorDb, thresholdDb, ratio);

            // --- ENVELOPE STAGE (Blueprint 3.2.2) ---
            // Smooth the gain changes. BallisticsFilter requires the channel index (ch).
            float smoothedGainDb = envelopeSmoother.processSample(ch, targetGainDb);
            float linearGain = juce::Decibels::decibelsToGain(smoothedGainDb);

            // --- APPLY GAIN & COLORATION ---
            float processedSample = inputSample * linearGain;

            // Apply coloration based on topology (WaveShaper takes 1 arg)
            processedSample = colorationStage.processSample(processedSample);

            // Apply makeup gain
            // FIX: dsp::Gain::processSample takes only 1 argument (the sample value).
            // The previous implementation incorrectly passed the channel index 'ch'.
            processedSample = makeupGain.processSample(processedSample);

            buffer.setSample(ch, i, processedSample);
        }
    }
}

================================================================================
// File: AdvancedCompressorProcessor.h
================================================================================

// File: FX_Modules/AdvancedCompressorProcessor.h
#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>
#include "../DSPUtils.h"

class AdvancedCompressorProcessor : public juce::AudioProcessor
{
public:
    // Topology selection (Blueprint 3.3)
    enum class Topology { VCA_Clean, FET_Aggressive, Opto_Smooth };
    // Detector Modes (Blueprint 3.2.1)
    enum class DetectorMode { Peak, RMS };

    AdvancedCompressorProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~AdvancedCompressorProcessor() override = default;

    const juce::String getName() const override { return "Advanced Compressor"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    // Standard JUCE Boilerplate
    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    double getTailLengthSeconds() const override { return 0.5; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}
private:
    // --- Detector Stage (Blueprint 3.2.1) ---
    juce::dsp::BallisticsFilter<float> peakDetector;

    // RMS detector implementation (Exponential Moving Average)
    float calculateRMS(int channel, float input);
    std::vector<float> rmsAverages;
    float rmsWindowTimeMs = 10.0f;
    float rmsAlpha = 0.99f;

    // --- Gain Computer ---
    float calculateGainDb(float detectorDb, float thresholdDb, float ratio);

    // --- Envelope Stage (Blueprint 3.2.2) ---
    juce::dsp::BallisticsFilter<float> envelopeSmoother;

    // --- Coloration stages (Blueprint 3.3) ---
    juce::dsp::WaveShaper<float> colorationStage;
    juce::dsp::Gain<float> makeupGain;

    // --- Parameters and State ---
    double currentSampleRate = 44100.0;

    void configureTopology(Topology topology, float attackMs, float releaseMs);

    juce::AudioProcessorValueTreeState& mainApvts;
    juce::String topologyParamId, detectorParamId, thresholdParamId, ratioParamId, attackParamId, releaseParamId, makeupParamId;
};

================================================================================
// File: AdvancedDelayProcessor.cpp
================================================================================

// File: FX_Modules/AdvancedDelayProcessor.cpp
#include "AdvancedDelayProcessor.h"

AdvancedDelayProcessor::AdvancedDelayProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_ADVDELAY_";
    modeParamId = slotPrefix + "MODE";
    timeParamId = slotPrefix + "TIME";
    feedbackParamId = slotPrefix + "FEEDBACK";
    mixParamId = slotPrefix + "MIX";
    colorParamId = slotPrefix + "COLOR";
    wowParamId = slotPrefix + "WOW";
    flutterParamId = slotPrefix + "FLUTTER";
    ageParamId = slotPrefix + "AGE";

    // Configure tape saturator (Blueprint 2.2.2)
    tapeSaturator.functionToUse = [](float x) { return std::tanh(x * 1.5f) * 0.9f; };
}

void AdvancedDelayProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    currentSampleRate = sampleRate;
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };

    delayLine.prepare(spec);
    delayLine.setMaximumDelayInSamples(static_cast<int>(sampleRate * 2.0));

    // Initialize Modulation Sources (Blueprint 2.2.1)
    wowLFO.prepare(spec);
    wowLFO.setWaveform(DSPUtils::LFO::Waveform::Sine);
    wowLFO.setFrequency(0.8f);

    flutterLFO.prepare(spec);
    flutterLFO.setWaveform(DSPUtils::LFO::Waveform::Triangle);
    flutterLFO.setFrequency(8.0f);

    noiseSource.setType(DSPUtils::NoiseGenerator::NoiseType::Pink);

    tapeFilters.prepare(spec);
    smoothedTimeMs.reset(sampleRate, 0.05);

    reset();
}

void AdvancedDelayProcessor::reset()
{
    delayLine.reset();
    wowLFO.reset();
    flutterLFO.reset();
    tapeFilters.reset();
    if (mainApvts.getRawParameterValue(timeParamId))
    {
        smoothedTimeMs.setCurrentAndTargetValue(mainApvts.getRawParameterValue(timeParamId)->load());
    }
}

void AdvancedDelayProcessor::releaseResources() {}

void AdvancedDelayProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&)
{
    juce::ScopedNoDenormals noDenormals;
    // P1 Boilerplate
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());

    // Safety check for parameters
    if (!mainApvts.getRawParameterValue(modeParamId) || !mainApvts.getRawParameterValue(timeParamId))
        return;

    auto mode = static_cast<DelayMode>(static_cast<int>(mainApvts.getRawParameterValue(modeParamId)->load()));
    smoothedTimeMs.setTargetValue(mainApvts.getRawParameterValue(timeParamId)->load());

    // Note: BBD mode (Blueprint 2.3) requires distinct filtering/companding.
    // For this implementation, Tape and BBD share the core logic in processTapeMode.
    if (mode == DelayMode::Digital)
    {
        processDigitalMode(buffer);
    }
    else
    {
        processTapeMode(buffer);
    }
}

// Implementation of the Tape Echo model (Blueprint 2.2)
void AdvancedDelayProcessor::processTapeMode(juce::AudioBuffer<float>& buffer)
{
    // Get Parameters (Safety check omitted for brevity, but essential)
    float feedback = mainApvts.getRawParameterValue(feedbackParamId)->load();
    float mix = mainApvts.getRawParameterValue(mixParamId)->load();
    float color = mainApvts.getRawParameterValue(colorParamId)->load();
    float wowDepth = mainApvts.getRawParameterValue(wowParamId)->load();
    float flutterDepth = mainApvts.getRawParameterValue(flutterParamId)->load();
    float age = mainApvts.getRawParameterValue(ageParamId)->load();

    // Configure Filters (Blueprint 2.2.2)
    tapeFilters.setType(juce::dsp::StateVariableTPTFilterType::lowpass);
    // Age slightly reduces the cutoff frequency
    float effectiveCutoff = color * (1.0f - age * 0.3f);
    tapeFilters.setCutoffFrequency(effectiveCutoff);

    int numSamples = buffer.getNumSamples();
    int numChannels = buffer.getNumChannels();

    // Process loop: Sample outer, Channel inner
    for (int i = 0; i < numSamples; ++i)
    {
        // 1. Calculate Modulation (Blueprint 2.2.1)
        // Scaled to max deviation in milliseconds (e.g., Wow 5ms, Flutter 1ms)
        float wowModMs = wowLFO.getNextBipolar() * wowDepth * 5.0f;
        float flutterModMs = flutterLFO.getNextBipolar() * flutterDepth * 1.0f;
        // Noise modulation based on age (scrape flutter/wear)
        float noiseModMs = noiseSource.getNextSample() * age * 0.5f;

        float totalModMs = wowModMs + flutterModMs + noiseModMs;

        // 2. Calculate Delay Time
        float currentTimeMs = smoothedTimeMs.getNextValue();
        float delayMs = juce::jmax(1.0f, currentTimeMs + totalModMs); // Ensure positive delay
        float delayInSamples = (float)(currentSampleRate * delayMs / 1000.0);
        delayInSamples = juce::jmin(delayInSamples, (float)delayLine.getMaximumDelayInSamples() - 1.0f);

        for (int ch = 0; ch < numChannels; ++ch)
        {
            float inputSample = buffer.getSample(ch, i);

            // 3. Read from Delay Line (Modulated)
            float delayedSample = delayLine.popSample(ch, delayInSamples, true);

            // 4. Apply Tape Degradation (Inside the feedback loop) (Blueprint 2.2.2)
            float processedWetSignal = delayedSample;

            // Apply Saturation
            processedWetSignal = tapeSaturator.processSample(processedWetSignal);

            // Apply Filtering
            processedWetSignal = tapeFilters.processSample(ch, processedWetSignal);

            // 5. Write to Delay Line (Feedback loop)
            float inputToDelay = inputSample + processedWetSignal * feedback;
            delayLine.pushSample(ch, inputToDelay);

            // 6. Mix Output
            float outputSample = (inputSample * (1.0f - mix)) + (processedWetSignal * mix);
            buffer.setSample(ch, i, outputSample);
        }
    }
}

void AdvancedDelayProcessor::processDigitalMode(juce::AudioBuffer<float>& buffer)
{
    // Implementation for clean digital delay (omitted for brevity)
    juce::ignoreUnused(buffer);
}

================================================================================
// File: AdvancedDelayProcessor.h
================================================================================

// File: FX_Modules/AdvancedDelayProcessor.h
#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>
#include "../DSPUtils.h"

class AdvancedDelayProcessor : public juce::AudioProcessor
{
public:
    enum class DelayMode { Tape, BBD, Digital };

    AdvancedDelayProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~AdvancedDelayProcessor() override = default;

    const juce::String getName() const override { return "Advanced Delay"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    // Standard JUCE Boilerplate
    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    double getTailLengthSeconds() const override { return 5.0; }
    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}
private:
    void processTapeMode(juce::AudioBuffer<float>& buffer);
    void processDigitalMode(juce::AudioBuffer<float>& buffer);

    // --- Core Components ---
    // Upgraded interpolation (Blueprint 2.2.1)
    juce::dsp::DelayLine<float, juce::dsp::DelayLineInterpolationTypes::Lagrange3rd> delayLine;
    double currentSampleRate = 44100.0;

    // --- Tape Mode Components (Blueprint 2.2) ---
    DSPUtils::LFO wowLFO;
    DSPUtils::LFO flutterLFO;
    DSPUtils::NoiseGenerator noiseSource;

    juce::dsp::WaveShaper<float> tapeSaturator;
    juce::dsp::StateVariableTPTFilter<float> tapeFilters;

    // --- Parameters ---
    juce::AudioProcessorValueTreeState& mainApvts;
    juce::String modeParamId, timeParamId, feedbackParamId, mixParamId, colorParamId, wowParamId, flutterParamId, ageParamId;

    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedTimeMs;
};

================================================================================
// File: BBDGranularEngine.cpp
================================================================================

//================================================================================
// File: FX_Modules/BBDGranularEngine.cpp
//================================================================================
#include "BBDGranularEngine.h"

BBDGranularEngine::BBDGranularEngine()
{
    auto timeSeed = static_cast<std::uintptr_t>(juce::Time::currentTimeMillis());
    auto addressSeed = reinterpret_cast<std::uintptr_t>(this);
    randomEngine.seed(static_cast<unsigned int>(timeSeed ^ addressSeed));
    noiseGen.setType(DSPUtils::NoiseGenerator::NoiseType::Pink);
}

void BBDGranularEngine::prepare(const juce::dsp::ProcessSpec& spec, const Config& newConfig, int maxBufferSizeSamples)
{
    sampleRate = spec.sampleRate;
    numChannels = (int)spec.numChannels;
    config = newConfig;

    captureBuffer.prepare(spec, maxBufferSizeSamples);

    grains.resize(MAX_GRAINS);
    juce::dsp::ProcessSpec monoSpec = spec;
    monoSpec.numChannels = 1;

    for (auto& grain : grains)
    {
        grain.filterL.prepare(monoSpec);
        grain.filterR.prepare(monoSpec);
        grain.filterL.setType(juce::dsp::StateVariableTPTFilterType::lowpass);
        grain.filterR.setType(juce::dsp::StateVariableTPTFilterType::lowpass);
    }

    reset();
}

void BBDGranularEngine::reset()
{
    captureBuffer.reset();
    for (auto& grain : grains)
    {
        grain.isActive = false;
        grain.filterL.reset();
        grain.filterR.reset();
    }
    samplesUntilNextGrain = 0.0f;
}

void BBDGranularEngine::capture(const juce::dsp::AudioBlock<float>& inputBlock)
{
    captureBuffer.write(inputBlock);
}

float BBDGranularEngine::Grain::applyTukeyWindow(float phase)
{
    const float alpha = 0.5f;
    if (phase < alpha / 2.0f)
        return 0.5f * (1.0f + std::cos(juce::MathConstants<float>::twoPi / alpha * (phase - alpha / 2.0f)));
    if (phase > 1.0f - alpha / 2.0f)
        return 0.5f * (1.0f + std::cos(juce::MathConstants<float>::twoPi / alpha * (phase - 1.0f + alpha / 2.0f)));
    return 1.0f;
}

void BBDGranularEngine::spawnGrain(float timeMs, float spread, float age)
{
    for (auto& grain : grains)
    {
        if (!grain.isActive)
        {
            grain.isActive = true;

            float durationMs = juce::jmap(distribution(randomEngine), config.minDurationMs, config.maxDurationMs);
            grain.durationSamples = static_cast<int>(sampleRate * durationMs / 1000.0);
            grain.grainPhase = 0.0f;

            float baseDelaySamples = (float)(sampleRate * timeMs / 1000.0);
            float jitter = (distribution(randomEngine) * 2.0f - 1.0f) * spread * baseDelaySamples * 0.5f;
            float actualDelaySamples = juce::jmax(10.0f, baseDelaySamples + jitter);

            int bufferSize = captureBuffer.getSize();
            if (bufferSize == 0) { grain.isActive = false; return; }

            grain.bufferReadPosition = (float)captureBuffer.getWritePosition() - actualDelaySamples;

            float normalizedTime = actualDelaySamples / (float)(sampleRate * 0.05);
            grain.pitchRatio = 1.0f / normalizedTime;
            grain.pitchRatio = juce::jlimit(0.1f, 5.0f, grain.pitchRatio);

            float baseCutoff = config.baseCutoffHz;
            baseCutoff *= (1.0f - age * 0.7f);

            float nyquist = (float)sampleRate * 0.5f;
            float aaCutoff = nyquist;

            if (grain.pitchRatio > 1.0f)
            {
                aaCutoff = nyquist / grain.pitchRatio;
            }
            else if (grain.pitchRatio < 1.0f)
            {
                aaCutoff = nyquist * grain.pitchRatio;
            }

            float cutoff = juce::jmin(baseCutoff, aaCutoff * 0.95f);
            cutoff = juce::jlimit(50.0f, nyquist - 50.0f, cutoff);

            grain.filterL.setCutoffFrequency(cutoff);
            grain.filterR.setCutoffFrequency(cutoff);
            grain.filterL.setResonance(0.707f);
            grain.filterR.setResonance(0.707f);
            grain.filterL.reset();
            grain.filterR.reset();

            grain.noiseLevel = age * config.noiseAmount;
            grain.amplitude = 0.7f + distribution(randomEngine) * 0.3f;
            grain.pan = distribution(randomEngine);

            return;
        }
    }
}

void BBDGranularEngine::process(juce::dsp::AudioBlock<float>& outputBlock, float density, float timeMs, float spread, float age)
{
    juce::ScopedNoDenormals noDenormals;

    int numSamples = (int)outputBlock.getNumSamples();
    float spawnRateHz = juce::jmap(density, 0.0f, config.spawnRateHzMax);
    if (spawnRateHz < 0.1f) spawnRateHz = 0.1f;
    float spawnIntervalSamples = (float)sampleRate / spawnRateHz;

    for (int i = 0; i < numSamples; ++i)
    {
        samplesUntilNextGrain -= 1.0f;
        if (samplesUntilNextGrain <= 0.0f)
        {
            spawnGrain(timeMs, spread, age);
            samplesUntilNextGrain += spawnIntervalSamples * (0.7f + distribution(randomEngine) * 0.6f);
        }

        for (auto& grain : grains)
        {
            if (grain.isActive)
            {
                float phase = grain.grainPhase / (float)grain.durationSamples;
                if (phase >= 1.0f)
                {
                    grain.isActive = false;
                    continue;
                }

                float window = Grain::applyTukeyWindow(phase);
                float gainL = window * std::cos(grain.pan * juce::MathConstants<float>::halfPi);
                float gainR = window * std::sin(grain.pan * juce::MathConstants<float>::halfPi);

                float sampleL = 0.0f, sampleR = 0.0f;

                if (numChannels > 0)
                    sampleL = captureBuffer.read(0, grain.bufferReadPosition);
                if (numChannels > 1)
                    sampleR = captureBuffer.read(1, grain.bufferReadPosition);
                else
                    sampleR = sampleL;

                sampleL += noiseGen.getNextSample() * grain.noiseLevel;
                sampleR += noiseGen.getNextSample() * grain.noiseLevel;

                float drive = config.saturationDrive * (1.0f + age * 0.5f);
                sampleL = DSPUtils::fastTanh(sampleL * drive);
                sampleR = DSPUtils::fastTanh(sampleR * drive);

                // JUCE 8 FIX (C2660): Pass channel index 0 as required by JUCE 8 API for mono filters.
                sampleL = grain.filterL.processSample(0, sampleL);
                sampleR = grain.filterR.processSample(0, sampleR);

                outputBlock.addSample(0, i, sampleL * gainL * grain.amplitude);
                if (numChannels > 1)
                    outputBlock.addSample(1, i, sampleR * gainR * grain.amplitude);

                grain.grainPhase += 1.0f;
                grain.bufferReadPosition += grain.pitchRatio;
            }
        }
    }
}

================================================================================
// File: BBDGranularEngine.h
================================================================================

//================================================================================
// File: FX_Modules/BBDGranularEngine.h
//================================================================================
#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>
#include <random>
#include <vector>
#include <cmath> // Added for std::cos
#include "../DSP_Helpers/InterpolatedCircularBuffer.h"
#include "../DSPUtils.h" // Required for NoiseGenerator and fastTanh
class BBDGranularEngine
{
public:
    // OPTIMIZATION: Set MAX_GRAINS to 64. This provides a good balance of density and performance
    // given the reduced spawn rates configured in BBDCloudProcessor.cpp.
    static constexpr int MAX_GRAINS = 64;

    // Configuration structure (remains the same)
    struct Config
    {
        float minDurationMs = 10.0f;
        float maxDurationMs = 100.0f;
        float baseCutoffHz = 5000.0f;
        float saturationDrive = 1.2f;
        float spawnRateHzMax = 500.0f;
        float noiseAmount = 0.05f;
    };

    BBDGranularEngine();
    void prepare(const juce::dsp::ProcessSpec& spec, const Config& newConfig, int maxBufferSizeSamples);
    void reset();
    void capture(const juce::dsp::AudioBlock<float>& inputBlock);
    void process(juce::dsp::AudioBlock<float>& outputBlock, float density, float timeMs, float spread, float age);

private:
    void spawnGrain(float timeMs, float spread, float age);

    struct Grain
    {
        bool isActive = false;
        int durationSamples = 0;
        float grainPhase = 0.0f; // 0.0 to 1.0
        float bufferReadPosition = 0.0f;

        // Stochastic Parameters
        float amplitude = 1.0f;
        float pan = 0.5f; // 0=L, 1=R

        // Per-Grain DSP (BBD Emulation)

        // ARTIFACT FIX: Upgraded from FirstOrderTPTFilter (6dB/oct) to StateVariableTPTFilter (12dB/oct).
        // Steeper filtering is essential for anti-aliasing in BBD emulation.
        // OLD: juce::dsp::FirstOrderTPTFilter<float> filterL, filterR;
        juce::dsp::StateVariableTPTFilter<float> filterL, filterR;

        float pitchRatio = 1.0f;
        float noiseLevel = 0.0f;

        static float applyTukeyWindow(float phase);
    };

    // --- Members ---
    double sampleRate = 44100.0;
    int numChannels = 2;
    Config config;
    InterpolatedCircularBuffer captureBuffer;
    std::vector<Grain> grains;

    // Spawning control
    float samplesUntilNextGrain = 0.0f;

    // Randomization
    std::minstd_rand randomEngine;
    std::uniform_real_distribution<float> distribution{ 0.0f, 1.0f };
    DSPUtils::NoiseGenerator noiseGen;
};

================================================================================
// File: ChromaTapeProcessor.cpp
================================================================================

//================================================================================
// File: FX_Modules/ChromaTapeProcessor.cpp
//================================================================================
#include "ChromaTapeProcessor.h"

// Constructor remains unchanged
ChromaTapeProcessor::ChromaTapeProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    // Parameter ID assignments (Unchanged)
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_CT_";
    saturationParamIds[0] = slotPrefix + "LOW_SATURATION";
    saturationParamIds[1] = slotPrefix + "MID_SATURATION";
    saturationParamIds[2] = slotPrefix + "HIGH_SATURATION";

    wowParamIds[0] = slotPrefix + "LOW_WOW";
    wowParamIds[1] = slotPrefix + "MID_WOW";
    wowParamIds[2] = slotPrefix + "HIGH_WOW";

    flutterParamIds[0] = slotPrefix + "LOW_FLUTTER";
    flutterParamIds[1] = slotPrefix + "MID_FLUTTER";
    flutterParamIds[2] = slotPrefix + "HIGH_FLUTTER";

    lowMidCrossoverParamId = slotPrefix + "LOWMID_CROSS";
    midHighCrossoverParamId = slotPrefix + "MIDHIGH_CROSS";

    // NEW: Assign IDs for new parameters
    scrapeParamId = slotPrefix + "SCRAPE_FLUTTER";
    chaosParamId = slotPrefix + "CHAOS_AMOUNT";
    hissParamId = slotPrefix + "HISS_LEVEL";
    humParamId = slotPrefix + "HUM_LEVEL";
    headBumpFreqParamId = slotPrefix + "HEADBUMP_FREQ";
    headBumpGainParamId = slotPrefix + "HEADBUMP_GAIN";
}

ChromaTapeProcessor::~ChromaTapeProcessor()
{
}

// Updated prepareToPlay
void ChromaTapeProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };
    juce::dsp::ProcessSpec monoSpec = spec;
    monoSpec.numChannels = 1;

    // 1. Prepare Bands
    for (int i = 0; i < NUM_BANDS; ++i)
    {
        auto& band = bands[i];
        band.saturator.prepare(spec);

        // NEW: Initialize Hysteresis State (Blueprint 2.3)
        band.hysteresis_last_input.resize(spec.numChannels, 0.0f);

        // NEW: Initialize EQ Stages (Blueprint 2.2, 2.4)
        band.headBumpFilters.resize(spec.numChannels);
        for (auto& filter : band.headBumpFilters) {
            filter.prepare(monoSpec);
        }

        band.dynamicHfFilter.prepare(spec);
        band.hfEnvelope.prepare(spec);

        if (i == HIGH)
        {
            band.dynamicHfFilter.setType(juce::dsp::StateVariableTPTFilterType::lowpass);
            band.hfEnvelope.setAttackTime(5.0f);
            band.hfEnvelope.setReleaseTime(50.0f);
        }

        // --- Modulation Initialization ---
        int maxDelaySamples = (int)(sampleRate * 0.030) + 2; // 30ms max delay
        band.delayLine.prepare(spec);
        band.delayLine.setMaximumDelayInSamples(maxDelaySamples);

        band.wowLFO.prepare(spec);
        band.flutterLFO.prepare(spec);
        band.wowLFO.setFrequency(1.0f);
        band.flutterLFO.setFrequency(15.0f);

        band.wowLFO.setStereoOffset(0.2f);
        band.flutterLFO.setStereoOffset(0.15f);

        // Noise/Smoothing setup
        band.noiseGen.setType(DSPUtils::NoiseGenerator::NoiseType::White);
        band.noiseFilter.prepare(monoSpec); // Use monoSpec (control signal)

        // OPTIMIZATION FIX: Initialize Stereo Mod Smoothers
        band.modSmoothers.resize(spec.numChannels);
        for (auto& smoother : band.modSmoothers)
        {
            smoother.prepare(monoSpec); // Prepare individual instances with monoSpec
            smoother.setCutoffFrequency(100.0f); // Example smoothing frequency
        }

        // Scrape Flutter Setup
        band.scrapeNoiseFilter.prepare(monoSpec); // Use monoSpec (control signal)
        band.scrapeNoiseFilter.setType(juce::dsp::StateVariableTPTFilterType::bandpass);
        band.scrapeNoiseFilter.setCutoffFrequency(3000.0f);
        band.scrapeNoiseFilter.setResonance(0.5f);

        // Initialize Parameter Smoothers
        band.smoothedWowDepth.reset(sampleRate, 0.05);
        band.smoothedFlutterDepth.reset(sampleRate, 0.05);
        band.smoothedSaturationDb.reset(sampleRate, 0.05);
    }

    // 2. Prepare Crossover Network
    crossover.prepare(spec);

    // 3. Prepare Noise/Hum
    hissGenerator.setType(DSPUtils::NoiseGenerator::NoiseType::Pink);
    hissShapingFilters.resize(spec.numChannels);
    auto hissCoeffs = juce::dsp::IIR::Coefficients<float>::makeHighShelf(sampleRate, 4000.0f, 0.5f, 6.0f);
    for (auto& filter : hissShapingFilters) {
        filter.prepare(monoSpec);
        *filter.coefficients = *hissCoeffs;
    }

    humOscillator.prepare(spec);
    humOscillator.setFrequency(60.0f);
    humHarmonicOscillator.prepare(spec);
    humHarmonicOscillator.setFrequency(180.0f);

    // 4. OPTIMIZATION FIX: Initialize Global Parameter Smoothers
    double smoothingTime = 0.05; // 50ms smoothing
    smoothedScrape.reset(sampleRate, smoothingTime);
    smoothedChaos.reset(sampleRate, smoothingTime);
    smoothedHissLevel.reset(sampleRate, smoothingTime);
    smoothedHumLevel.reset(sampleRate, smoothingTime);

    reset();
}

void ChromaTapeProcessor::releaseResources()
{
    reset();
}

// Updated reset
void ChromaTapeProcessor::reset()
{
    for (auto& band : bands)
    {
        band.saturator.reset();
        band.delayLine.reset();
        band.wowLFO.reset();
        band.flutterLFO.reset();
        band.noiseFilter.reset();

        // OPTIMIZATION FIX: Reset Stereo Mod Smoothers
        for (auto& smoother : band.modSmoothers)
        {
            smoother.reset();
        }

        for (auto& filter : band.headBumpFilters) filter.reset();
        band.dynamicHfFilter.reset();
        band.hfEnvelope.reset();
        std::fill(band.hysteresis_last_input.begin(), band.hysteresis_last_input.end(), 0.0f);
        band.scrapeNoiseFilter.reset();
        band.chaos_state = 0.5f;

        band.smoothedWowDepth.setCurrentAndTargetValue(band.smoothedWowDepth.getTargetValue());
        band.smoothedFlutterDepth.setCurrentAndTargetValue(band.smoothedFlutterDepth.getTargetValue());
        band.smoothedSaturationDb.setCurrentAndTargetValue(band.smoothedSaturationDb.getTargetValue());
    }

    crossover.reset();
    for (auto& filter : hissShapingFilters) filter.reset();
    humOscillator.reset();
    humHarmonicOscillator.reset();

    // OPTIMIZATION FIX: Reset Global Smoothers
    smoothedScrape.setCurrentAndTargetValue(smoothedScrape.getTargetValue());
    smoothedChaos.setCurrentAndTargetValue(smoothedChaos.getTargetValue());
    smoothedHissLevel.setCurrentAndTargetValue(smoothedHissLevel.getTargetValue());
    smoothedHumLevel.setCurrentAndTargetValue(smoothedHumLevel.getTargetValue());
}

// Constants
const float MAX_GAIN_LINEAR = 4.0f;
const float LOW_ASYMMETRY = 0.7f;
const float MID_ASYMMETRY_OFFSET = 0.1f;
const float HIGH_ASYMMETRY = 0.1f;

static float calculateInternalDrive(float saturationDb)
{
    if (saturationDb <= 0.01f) return 0.0f;
    float driveLinear = juce::Decibels::decibelsToGain(saturationDb);
    float internalDrive = (driveLinear - 1.0f) / (MAX_GAIN_LINEAR - 1.0f);
    return juce::jlimit(0.0f, 1.0f, internalDrive);
}

// Updated updateParameters
void ChromaTapeProcessor::updateParameters()
{
    // Added safety checks for parameter existence
    if (!mainApvts.getRawParameterValue(lowMidCrossoverParamId) || !mainApvts.getRawParameterValue(midHighCrossoverParamId)) return;

    float lowMidCross = mainApvts.getRawParameterValue(lowMidCrossoverParamId)->load();
    float midHighCross = mainApvts.getRawParameterValue(midHighCrossoverParamId)->load();
    crossover.setCrossoverFrequencies(lowMidCross, midHighCross);

    for (int i = 0; i < NUM_BANDS; ++i)
    {
        // Added safety checks
        if (mainApvts.getRawParameterValue(saturationParamIds[i]))
            bands[i].smoothedSaturationDb.setTargetValue(mainApvts.getRawParameterValue(saturationParamIds[i])->load());
        if (mainApvts.getRawParameterValue(wowParamIds[i]))
            bands[i].smoothedWowDepth.setTargetValue(mainApvts.getRawParameterValue(wowParamIds[i])->load());
        if (mainApvts.getRawParameterValue(flutterParamIds[i]))
            bands[i].smoothedFlutterDepth.setTargetValue(mainApvts.getRawParameterValue(flutterParamIds[i])->load());
    }

    // OPTIMIZATION FIX: Update Global Smoothers (Read APVTS once per block)
    if (mainApvts.getRawParameterValue(scrapeParamId))
        smoothedScrape.setTargetValue(mainApvts.getRawParameterValue(scrapeParamId)->load());
    if (mainApvts.getRawParameterValue(chaosParamId))
        smoothedChaos.setTargetValue(mainApvts.getRawParameterValue(chaosParamId)->load());

    if (mainApvts.getRawParameterValue(hissParamId)) {
        float hissDb = mainApvts.getRawParameterValue(hissParamId)->load();
        smoothedHissLevel.setTargetValue(juce::Decibels::decibelsToGain(hissDb));
    }
    if (mainApvts.getRawParameterValue(humParamId)) {
        float humDb = mainApvts.getRawParameterValue(humParamId)->load();
        smoothedHumLevel.setTargetValue(juce::Decibels::decibelsToGain(humDb));
    }

    // Check sample rate validity before calculating coefficients
    if (!bands[LOW].headBumpFilters.empty() && getSampleRate() > 0 && mainApvts.getRawParameterValue(headBumpFreqParamId) && mainApvts.getRawParameterValue(headBumpGainParamId))
    {
        float headBumpFreq = mainApvts.getRawParameterValue(headBumpFreqParamId)->load();
        float headBumpGain = mainApvts.getRawParameterValue(headBumpGainParamId)->load();
        auto headBumpCoeffs = juce::dsp::IIR::Coefficients<float>::makePeakFilter(getSampleRate(), headBumpFreq, 0.7f, juce::Decibels::decibelsToGain(headBumpGain));
        for (auto& filter : bands[LOW].headBumpFilters) {
            *filter.coefficients = *headBumpCoeffs;
        }
    }
}

// Updated processBlock
void ChromaTapeProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&)
{
    juce::ScopedNoDenormals noDenormals;
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());

    int numSamples = buffer.getNumSamples();
    int numChannels = buffer.getNumChannels();

    // 1. Update Parameters
    updateParameters();

    // 2. Split into bands
    crossover.processBlock(buffer);

    // 3. Process each band
    std::array<juce::AudioBuffer<float>*, NUM_BANDS> bandBuffers = {
        &crossover.getLowBand(), &crossover.getMidBand(), &crossover.getHighBand()
    };

    // Main Processing Loop
    for (int sample = 0; sample < numSamples; ++sample)
    {
        // OPTIMIZATION FIX: Advance global smoothers once per sample frame
        smoothedScrape.getNextValue();
        smoothedChaos.getNextValue();

        for (int i = 0; i < NUM_BANDS; ++i)
        {
            bands[i].smoothedSaturationDb.getNextValue();
            bands[i].smoothedWowDepth.getNextValue();
            bands[i].smoothedFlutterDepth.getNextValue();

            updateModulation(i);

            processBand(i, sample, numChannels, *bandBuffers[i]);
        }
    }

    // 4. Sum bands back into output buffer
    buffer.clear();
    for (int i = 0; i < NUM_BANDS; ++i)
    {
        auto* bandBuffer = bandBuffers[i];
        for (int ch = 0; ch < numChannels; ++ch)
        {
            if (ch < bandBuffer->getNumChannels())
            {
                buffer.addFrom(ch, 0, *bandBuffer, ch, 0, numSamples);
            }
        }
    }

    // 5. Inject Hiss (Blueprint IV.1)
    if (smoothedHissLevel.getTargetValue() > 1e-6f && hissShapingFilters.size() == (size_t)numChannels)
    {
        for (int sample = 0; sample < numSamples; ++sample)
        {
            float currentHissLevel = smoothedHissLevel.getNextValue();
            float noise = hissGenerator.getNextSample();

            for (int ch = 0; ch < numChannels; ++ch)
            {
                // CONFIRMED: IIR::Filter prepared with monoSpec uses 1 argument.
                float hissSample = hissShapingFilters[ch].processSample(noise) * currentHissLevel;
                buffer.addSample(ch, sample, hissSample);
            }
        }
    }
    else if (smoothedHissLevel.isSmoothing())
    {
        smoothedHissLevel.skip(numSamples);
    }
}

// The Core Processing Logic (Optimized Structure)
void ChromaTapeProcessor::processBand(int bandIdx, int sample, int numChannels, juce::AudioBuffer<float>& buffer)
{
    auto& band = bands[bandIdx];
    float saturationDb = band.smoothedSaturationDb.getCurrentValue();
    float internalDrive = calculateInternalDrive(saturationDb);

    float hum = 0.0f;
    if (bandIdx == LOW)
    {
        float currentHumLevel = smoothedHumLevel.getNextValue();
        hum = (humOscillator.getNextBipolar() + humHarmonicOscillator.getNextBipolar() * 0.5f) * currentHumLevel;
    }

    constexpr int MAX_CHANNELS = 2;
    std::array<float, MAX_CHANNELS> sat_outputs;
    int activeChannels = juce::jmin(numChannels, MAX_CHANNELS);

    // === Stages 1 & 2: EQ and Saturation (Per Channel) ===
    for (int ch = 0; ch < activeChannels; ++ch)
    {
        float input = buffer.getSample(ch, sample);

        if (bandIdx == LOW)
        {
            input += hum;
            if (ch < (int)band.headBumpFilters.size())
            {
                // CONFIRMED: IIR::Filter prepared with monoSpec uses 1 argument.
                input = band.headBumpFilters[ch].processSample(input);
            }
        }

        float sat_out = input;
        if (internalDrive > 0.0f)
        {
            band.saturator.setDrive(internalDrive);

            if (bandIdx == LOW)
            {
                band.saturator.setAsymmetry(LOW_ASYMMETRY);
            }
            else if (bandIdx == MID)
            {
                if (ch < (int)band.hysteresis_last_input.size())
                {
                    if (input > band.hysteresis_last_input[ch])
                        band.saturator.setAsymmetry(MID_ASYMMETRY_OFFSET);
                    else
                        band.saturator.setAsymmetry(-MID_ASYMMETRY_OFFSET);

                    band.hysteresis_last_input[ch] = input;
                }
            }
            else
            {
                band.saturator.setAsymmetry(HIGH_ASYMMETRY);
            }

            sat_out = band.saturator.processSample(ch, input);

            if (saturationDb > 1e-6f) {
                sat_out *= (1.0f / juce::Decibels::decibelsToGain(saturationDb));
            }
        }

        sat_outputs[ch] = sat_out;
    }

    // === Stage 2.5: Calculate Envelope (OPTIMIZATION: Once per frame) ===
    if (bandIdx == HIGH)
    {
        float maxAbsSat = 0.0f;
        for (int ch = 0; ch < activeChannels; ++ch)
        {
            maxAbsSat = juce::jmax(maxAbsSat, std::abs(sat_outputs[ch]));
        }

        float envelope = band.hfEnvelope.process(maxAbsSat);
        float cutoffHz = juce::jmap(juce::jlimit(0.0f, 0.5f, envelope), 0.0f, 0.5f, 20000.0f, 6000.0f);
        band.dynamicHfFilter.setCutoffFrequency(cutoffHz);
    }

    // === Stages 3 & 4: Post-Filtering and Modulation (Per Channel) ===
    for (int ch = 0; ch < activeChannels; ++ch)
    {
        float processed_sample = sat_outputs[ch];

        if (bandIdx == HIGH)
        {
            processed_sample = band.dynamicHfFilter.processSample(ch, processed_sample);
        }

        float mod_out = applyModulation(bandIdx, ch, processed_sample);
        buffer.setSample(ch, sample, mod_out);
    }
}

// Blueprint III - Update LFOs, Chaos, and Noise (Once per frame per band)
void ChromaTapeProcessor::updateModulation(int bandIdx)
{
    auto& band = bands[bandIdx];

    band.chaos_state = band.CHAOS_R * band.chaos_state * (1.0f - band.chaos_state);

    float chaosAmount = smoothedChaos.getCurrentValue();

    if (chaosAmount > 0.001f)
    {
        float chaos_bipolar = (band.chaos_state * 2.0f - 1.0f) * chaosAmount;
        float wowFreq = 1.0f * (1.0f + chaos_bipolar * 0.2f);
        float flutterFreq = 15.0f * (1.0f + chaos_bipolar * 0.2f);
        band.wowLFO.setFrequency(wowFreq);
        band.flutterLFO.setFrequency(flutterFreq);
    }

    band.currentWow = band.wowLFO.getNextStereoSample();
    band.currentFlutter = band.flutterLFO.getNextStereoSample();

    float noise = band.noiseGen.getNextSample();

    // FIX: TPT and SVT filters require the channel index (0 for mono).
    band.currentFilteredNoise = band.noiseFilter.processSample(0, noise);
    band.currentScrapeNoise = band.scrapeNoiseFilter.processSample(0, noise);
}

// Blueprint III - Calculate and Apply Delay (Optimized)
float ChromaTapeProcessor::applyModulation(int bandIdx, int channel, float inputSample)
{
    auto& band = bands[bandIdx];
    float sr = (float)getSampleRate();

    const float MAX_WOW_MS = 10.0f;
    const float MAX_FLUTTER_MS = 2.0f;
    const float MAX_SCRAPE_MS = 0.5f;
    const float BASE_DELAY_MS = 15.0f;

    float wowDepth = band.smoothedWowDepth.getCurrentValue();
    float flutterDepth = band.smoothedFlutterDepth.getCurrentValue();
    float scrapeDepth = smoothedScrape.getCurrentValue();

    float wowMod = (channel == 0) ? band.currentWow.first : band.currentWow.second;
    float periodicFlutter = (channel == 0) ? band.currentFlutter.first : band.currentFlutter.second;

    float wowModMs = wowMod * wowDepth * MAX_WOW_MS * 0.5f;
    float flutterModMs = (periodicFlutter * 0.7f + band.currentFilteredNoise * 0.3f) * flutterDepth * MAX_FLUTTER_MS * 0.5f;
    float scrapeModMs = band.currentScrapeNoise * scrapeDepth * MAX_SCRAPE_MS;

    float rawModulationMs = BASE_DELAY_MS + wowModMs + flutterModMs + scrapeModMs;

    float smoothedDelayMs = rawModulationMs;
    if (channel < (int)band.modSmoothers.size())
    {
        // FIX: FirstOrderTPTFilter requires the channel index (0 for mono).
        smoothedDelayMs = band.modSmoothers[channel].processSample(0, rawModulationMs);
    }

    float delaySamples = smoothedDelayMs * sr / 1000.0f;
    delaySamples = juce::jmax(0.1f, juce::jmin(delaySamples, (float)band.delayLine.getMaximumDelayInSamples() - 1.0f));

    band.delayLine.pushSample(channel, inputSample);
    return band.delayLine.popSample(channel, delaySamples, true);
}


// Crossover Network Implementation
void ChromaTapeProcessor::CrossoverNetwork::prepare(const juce::dsp::ProcessSpec& spec)
{
    lowMidLowpass.prepare(spec);
    lowMidHighpass.prepare(spec);
    midHighLowpass.prepare(spec);
    midHighHighpass.prepare(spec);

    lowMidLowpass.setType(juce::dsp::LinkwitzRileyFilterType::lowpass);
    lowMidHighpass.setType(juce::dsp::LinkwitzRileyFilterType::highpass);
    midHighLowpass.setType(juce::dsp::LinkwitzRileyFilterType::lowpass);
    midHighHighpass.setType(juce::dsp::LinkwitzRileyFilterType::highpass);

    int numChannels = (int)spec.numChannels;
    int maxSamples = (int)spec.maximumBlockSize;

    lowBand.setSize(numChannels, maxSamples);
    midBand.setSize(numChannels, maxSamples);
    highBand.setSize(numChannels, maxSamples);
}

void ChromaTapeProcessor::CrossoverNetwork::reset()
{
    lowMidLowpass.reset();
    lowMidHighpass.reset();
    midHighLowpass.reset();
    midHighHighpass.reset();
}

void ChromaTapeProcessor::CrossoverNetwork::setCrossoverFrequencies(float lowMid, float midHigh)
{
    midHigh = juce::jmax(lowMid + 20.0f, midHigh);

    lowMidLowpass.setCutoffFrequency(lowMid);
    lowMidHighpass.setCutoffFrequency(lowMid);
    midHighLowpass.setCutoffFrequency(midHigh);
    midHighHighpass.setCutoffFrequency(midHigh);
}

void ChromaTapeProcessor::CrossoverNetwork::processBlock(juce::AudioBuffer<float>& buffer)
{
    int numChannels = buffer.getNumChannels();
    int numSamples = buffer.getNumSamples();

    // Ensure buffers are correctly sized
    lowBand.setSize(numChannels, numSamples, false, false, true);
    midBand.setSize(numChannels, numSamples, false, false, true);
    highBand.setSize(numChannels, numSamples, false, false, true);

    // Process Low Band
    lowBand.makeCopyOf(buffer);
    juce::dsp::AudioBlock<float> lowBlock(lowBand);
    juce::dsp::ProcessContextReplacing<float> lowContext(lowBlock);
    lowMidLowpass.process(lowContext);

    // Process High Band (initially contains input signal)
    highBand.makeCopyOf(buffer);
    juce::dsp::AudioBlock<float> highBlock(highBand);
    juce::dsp::ProcessContextReplacing<float> highContext(highBlock);
    lowMidHighpass.process(highContext); // highBand now contains frequencies > lowMid

    // Process Mid Band (copy from the partially filtered highBand)
    midBand.makeCopyOf(highBand);
    juce::dsp::AudioBlock<float> midBlock(midBand);
    juce::dsp::ProcessContextReplacing<float> midContext(midBlock);
    midHighLowpass.process(midContext); // midBand now contains frequencies between lowMid and midHigh

    // Finalize High Band
    midHighHighpass.process(highContext); // highBand now contains frequencies > midHigh
}

================================================================================
// File: ChromaTapeProcessor.h
================================================================================

//================================================================================
// File: FX_Modules/ChromaTapeProcessor.h (REVISED)
//================================================================================
#pragma once
#include <JuceHeader.h>
#include "../DSPUtils.h"
// CHANGED: Include the optimized saturation model
#include "TapeSaturation.h"

class ChromaTapeProcessor : public juce::AudioProcessor
{
public:
    ChromaTapeProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~ChromaTapeProcessor() override;

    const juce::String getName() const override { return "ChromaTape"; }

    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    // Boilerplate methods (unchanged)
    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    double getTailLengthSeconds() const override { return 0.5; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

private:
    static constexpr int NUM_BANDS = 3;
    enum BandIndex { LOW = 0, MID = 1, HIGH = 2 };

    //==============================================================================
    // Revised TapeBand Structure (Optimized)
    //==============================================================================
    struct TapeBand
    {
        // --- Saturation & Core ---
        TapeDSP::OptimizedTapeSaturator saturator;
        juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedSaturationDb;

        // NEW (Blueprint II.3): State for Mid Band hysteresis model (per channel)
        std::vector<float> hysteresis_last_input;

        // NEW (Blueprint II.2, II.4): EQ Stages

        // Low Band: IIR Peak Filter for Head Bump. We need per-channel instances.
        using EQFilter = juce::dsp::IIR::Filter<float>;
        std::vector<EQFilter> headBumpFilters;

        // High Band: TPT LPF for Dynamic HF Loss.
        juce::dsp::StateVariableTPTFilter<float> dynamicHfFilter;
        DSPUtils::EnvelopeFollower hfEnvelope;

        // --- Mechanical Degradation Stage (Wow & Flutter) ---
        juce::dsp::DelayLine<float, juce::dsp::DelayLineInterpolationTypes::Linear> delayLine;
        DSPUtils::LFO wowLFO;
        DSPUtils::LFO flutterLFO;
        DSPUtils::NoiseGenerator noiseGen;
        juce::dsp::FirstOrderTPTFilter<float> noiseFilter; // Mono control signal filter

        // OPTIMIZATION FIX: Modulation smoother must be stereo.
        std::vector<juce::dsp::FirstOrderTPTFilter<float>> modSmoothers;

        // Scrape Flutter (Mono control signal filter)
        juce::dsp::StateVariableTPTFilter<float> scrapeNoiseFilter;

        // NEW (Blueprint III.3): Chaotic Modulator
        float chaos_state{ 0.5f };
        static constexpr float CHAOS_R = 3.9f; // Logistic map parameter for chaos

        // Parameter smoothing (smoothedWowDepth, smoothedFlutterDepth remain)
        juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedWowDepth;
        juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedFlutterDepth;

        // Storage for stereo modulation values (LFOs)
        std::pair<float, float> currentWow = { 0.0f, 0.0f };
        std::pair<float, float> currentFlutter = { 0.0f, 0.0f };

        // OPTIMIZATION FIX: Storage for noise components (advanced once per frame)
        float currentFilteredNoise = 0.0f;
        float currentScrapeNoise = 0.0f;
    };

    std::array<TapeBand, NUM_BANDS> bands;

    // Crossover Network (Structure unchanged)
    struct CrossoverNetwork
    {
        juce::dsp::LinkwitzRileyFilter<float> lowMidLowpass;
        juce::dsp::LinkwitzRileyFilter<float> lowMidHighpass;
        juce::dsp::LinkwitzRileyFilter<float> midHighLowpass;
        juce::dsp::LinkwitzRileyFilter<float> midHighHighpass;

        juce::AudioBuffer<float> lowBand, midBand, highBand;

        void prepare(const juce::dsp::ProcessSpec& spec);
        void reset();
        void setCrossoverFrequencies(float lowMid, float midHigh);
        void processBlock(juce::AudioBuffer<float>& buffer);

        juce::AudioBuffer<float>& getLowBand() { return lowBand; }
        juce::AudioBuffer<float>& getMidBand() { return midBand; }
        juce::AudioBuffer<float>& getHighBand() { return highBand; }
    };

    CrossoverNetwork crossover;

    // NEW (Blueprint IV): Noise and Hum Generators
    DSPUtils::NoiseGenerator hissGenerator;
    // Hiss shaping (High Shelf). Need per-channel instances.
    std::vector<juce::dsp::IIR::Filter<float>> hissShapingFilters;

    DSPUtils::LFO humOscillator;
    DSPUtils::LFO humHarmonicOscillator;

    // OPTIMIZATION FIX: Global Parameter Smoothers
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedScrape;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedChaos;
    // Use Multiplicative smoothing for gain levels (Hiss/Hum)
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Multiplicative> smoothedHissLevel;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Multiplicative> smoothedHumLevel;

    juce::AudioProcessorValueTreeState& mainApvts;
    std::array<juce::String, NUM_BANDS> saturationParamIds;
    std::array<juce::String, NUM_BANDS> wowParamIds;
    std::array<juce::String, NUM_BANDS> flutterParamIds;
    juce::String lowMidCrossoverParamId, midHighCrossoverParamId;

    // NEW: Parameter IDs
    juce::String scrapeParamId, chaosParamId, hissParamId, humParamId;
    juce::String headBumpFreqParamId, headBumpGainParamId;

    // NEW: Helper methods for the refactored processing
    void updateParameters();
    void processBand(int bandIdx, int sample, int numChannels, juce::AudioBuffer<float>& buffer);
    void updateModulation(int bandIdx);
    float applyModulation(int bandIdx, int channel, float inputSample);
};

================================================================================
// File: CompressorProcessor.cpp
================================================================================

#include "CompressorProcessor.h"

CompressorProcessor::CompressorProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_";
    typeParamId = slotPrefix + "COMP_TYPE";
    thresholdParamId = slotPrefix + "COMP_THRESHOLD";
    ratioParamId = slotPrefix + "COMP_RATIO";
    attackParamId = slotPrefix + "COMP_ATTACK";
    releaseParamId = slotPrefix + "COMP_RELEASE";
    makeupParamId = slotPrefix + "COMP_MAKEUP";
}

void CompressorProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };
    compressor.prepare(spec);
    makeupGain.prepare(spec);
    reset();
}

void CompressorProcessor::releaseResources()
{
    reset();
}

void CompressorProcessor::reset()
{
    compressor.reset();
    makeupGain.reset();
}

void CompressorProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&)
{
    juce::ScopedNoDenormals noDenormals;

    // === FIX P1: Add standard boilerplate ===
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();

    // Clear any output channels that didn't contain input data
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());
    // ========================================

    auto type = static_cast<int>(mainApvts.getRawParameterValue(typeParamId)->load());

    compressor.setThreshold(mainApvts.getRawParameterValue(thresholdParamId)->load());
    compressor.setRatio(mainApvts.getRawParameterValue(ratioParamId)->load());
    compressor.setAttack(mainApvts.getRawParameterValue(attackParamId)->load());
    compressor.setRelease(mainApvts.getRawParameterValue(releaseParamId)->load());
    makeupGain.setGainDecibels(mainApvts.getRawParameterValue(makeupParamId)->load());

    juce::dsp::AudioBlock<float> block(buffer);
    juce::dsp::ProcessContextReplacing<float> context(block);
    compressor.process(context);
    makeupGain.process(context);

    if (type == 1) // FET
    {
        for (int channel = 0; channel < buffer.getNumChannels(); ++channel)
        {
            auto* channelData = buffer.getWritePointer(channel);
            for (int i = 0; i < buffer.getNumSamples(); ++i)
                channelData[i] = std::tanh(channelData[i] * 1.2f);
        }
    }
}

================================================================================
// File: CompressorProcessor.h
================================================================================

#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>

class CompressorProcessor : public juce::AudioProcessor
{
public:
    CompressorProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~CompressorProcessor() override = default;

    const juce::String getName() const override { return "Compressor"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }

    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    double getTailLengthSeconds() const override { return 0.0; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

private:
    juce::dsp::Compressor<float> compressor;
    juce::dsp::Gain<float> makeupGain;

    juce::AudioProcessorValueTreeState& mainApvts;
    juce::String typeParamId, thresholdParamId, ratioParamId, attackParamId, releaseParamId, makeupParamId;
};

================================================================================
// File: DelayProcessor.cpp
================================================================================

//================================================================================
// File: FX_Modules/DelayProcessor.cpp
//================================================================================
#include "DelayProcessor.h"

DelayProcessor::DelayProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_";
    typeParamId = slotPrefix + "DELAY_TYPE";
    timeParamId = slotPrefix + "DELAY_TIME";
    feedbackParamId = slotPrefix + "DELAY_FEEDBACK";
    mixParamId = slotPrefix + "DELAY_MIX";
    dampingParamId = slotPrefix + "DELAY_DAMPING";
}

void DelayProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };
    delayLine.prepare(spec);
    delayLine.setMaximumDelayInSamples(static_cast<int>(sampleRate * 2.0));
    feedbackFilter.prepare(spec);
    feedbackFilter.setType(juce::dsp::StateVariableTPTFilterType::lowpass);
    reset();
}

void DelayProcessor::releaseResources()
{
    reset();
}

void DelayProcessor::reset()
{
    delayLine.reset();
    feedbackFilter.reset();
}


// Replace processBlock entirely:
void DelayProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer& midiMessages) {
    // FIX P1: Boilerplate
    juce::ScopedNoDenormals noDenormals;
    // FIX C4100
    juce::ignoreUnused(midiMessages);
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());
    // Get parameters
    auto type = static_cast<int>(mainApvts.getRawParameterValue(typeParamId)->load());
    float time = mainApvts.getRawParameterValue(timeParamId)->load();
    float feedback = mainApvts.getRawParameterValue(feedbackParamId)->load();
    float mix = mainApvts.getRawParameterValue(mixParamId)->load();
    float damping = mainApvts.getRawParameterValue(dampingParamId)->load();

    float delaySamples = (float)(getSampleRate() * time / 1000.0);
    feedbackFilter.setCutoffFrequency(damping);
    // FIX P3: Corrected feedback loop
    for (int channel = 0; channel < buffer.getNumChannels(); ++channel)
    {
        // ✅ FINAL FIX: The filter was correctly prepared for stereo, so we just need to process each channel.
        // The previous check was incorrect because StateVariableTPTFilter *does not* have getNumChannels().
        auto* channelData = buffer.getWritePointer(channel);
        for (int i = 0; i < buffer.getNumSamples(); ++i)
        {
            float inputSample = channelData[i];
            // 1. Get the delayed sample
            float delayedSample = delayLine.popSample(channel, delaySamples, true);
            // 2. Apply damping filter to the delayed signal
            float filteredDelayedSample = feedbackFilter.processSample(channel, delayedSample);
            // 3. Apply saturation if analog mode
            if (type == 1) // Analog BBD
            {
                filteredDelayedSample = std::tanh(filteredDelayedSample);
            }

            // 4. Calculate feedback amount
            float feedbackSample = filteredDelayedSample * feedback;
            // 5. Calculate input to the delay line
            float inputToDelay = inputSample + feedbackSample;
            // 6. Push into the delay line
            delayLine.pushSample(channel, inputToDelay);
            // 7. Calculate output mix (Use the processed signal)
            channelData[i] = (inputSample * (1.0f - mix)) + (filteredDelayedSample * mix);
        }
    }
}

================================================================================
// File: DelayProcessor.h
================================================================================

#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>

class DelayProcessor : public juce::AudioProcessor
{
public:
    DelayProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~DelayProcessor() override = default;

    const juce::String getName() const override { return "Delay"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    double getTailLengthSeconds() const override { return 2.0; }

    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

private:
    juce::dsp::DelayLine<float, juce::dsp::DelayLineInterpolationTypes::Linear> delayLine;
    juce::dsp::StateVariableTPTFilter<float> feedbackFilter;
    // float lastFeedbackOutput[2] = { 0.0f, 0.0f }; // REMOVE THIS LINE

    juce::AudioProcessorValueTreeState& mainApvts;
    juce::String typeParamId, timeParamId, feedbackParamId, mixParamId, dampingParamId;
};

================================================================================
// File: DistortionProcessor.cpp
================================================================================

﻿//================================================================================
// File: FX_Modules/DistortionProcessor.cpp (CORRECTED)
//================================================================================

#include "DistortionProcessor.h"

// FIX: Removed incorrect initialization of ProcessorDuplicator from the initializer list.
// Initialization must rely on prepareToPlay where the actual sample rate is known.
DistortionProcessor::DistortionProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
    // REMOVED:
    // inputDCBlocker(juce::dsp::IIR::Coefficients<float>::makeHighPass(44100.0, 20.0f)),
    // outputDCBlocker(juce::dsp::IIR::Coefficients<float>::makeHighPass(44100.0, 20.0f))
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_";
    driveParamId = slotPrefix + "DISTORTION_DRIVE";
    levelParamId = slotPrefix + "DISTORTION_LEVEL";
    typeParamId = slotPrefix + "DISTORTION_TYPE";
    biasParamId = slotPrefix + "DISTORTION_BIAS";
    characterParamId = slotPrefix + "DISTORTION_CHARACTER";
}

// The rest of the DistortionProcessor.cpp file remains the same as your provided version,
// as the prepareToPlay implementation was already handling initialization correctly.

void DistortionProcessor::prepareToPlay(double sampleRate, int samplesPerBlock) {
    auto numChannels = (juce::uint32)getTotalNumInputChannels();
    if (numChannels == 0) numChannels = 2;

    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, numChannels };

    preGain.prepare(spec);
    postGain.prepare(spec);
    preGain.setRampDurationSeconds(0.01);
    postGain.setRampDurationSeconds(0.01);

    // === FIX: Initialize the ProcessorDuplicator for DC Blocking (High-pass at 20Hz) ===
    inputDCBlocker.prepare(spec);
    outputDCBlocker.prepare(spec);

    if (sampleRate > 0)
    {
        // Calculate coefficients (returns a ReferenceCountedPointer)
        auto dcBlockerCoeffs = juce::dsp::IIR::Coefficients<float>::makeHighPass(sampleRate, 20.0f);
        // Assign to the shared state of the ProcessorDuplicator
        *inputDCBlocker.state = *dcBlockerCoeffs;
        *outputDCBlocker.state = *dcBlockerCoeffs;
    }
    // =====================================================================================

    double smoothingTime = 0.05;
    smoothedBias.reset(sampleRate, smoothingTime);
    smoothedCharacter.reset(sampleRate, smoothingTime);

    inputFollower.prepare(spec);
    inputFollower.setAttackTime(5.0f);
    inputFollower.setReleaseTime(50.0f);

    localOversampler = std::make_unique<juce::dsp::Oversampling<float>>(
        static_cast<size_t>(numChannels), 2, juce::dsp::Oversampling<float>::filterHalfBandFIREquiripple, true);
    localOversampler->initProcessing(static_cast<size_t>(samplesPerBlock));

    reset();
}

void DistortionProcessor::releaseResources() {
    reset();
}

void DistortionProcessor::reset() {
    preGain.reset();
    postGain.reset();
    // === FIX: Reset the ProcessorDuplicators ===
    inputDCBlocker.reset();
    outputDCBlocker.reset();
    // ===========================================
    inputFollower.reset();
    if (localOversampler)
        localOversampler->reset();
    smoothedBias.setCurrentAndTargetValue(0.0f);
    smoothedCharacter.setCurrentAndTargetValue(0.5f);
}

float DistortionProcessor::processTube(float x, float bias, float dynamicBias) {
    float effectiveBias = bias * 0.5f + (dynamicBias * 0.3f);
    float y = x + effectiveBias;
    if (y > 0)
        return std::tanh(y * 0.9f);
    else
        return std::tanh(y * 1.4f);
}

float DistortionProcessor::processOpAmp(float x, float character) {
    float soft = std::tanh(x * 1.5f);
    float hard = x / (std::abs(x) + 0.6f) * 0.8f;
    return juce::jmap(character, hard, soft);
}

float DistortionProcessor::processGermanium(float x, float stability) {
    float gateThreshold = juce::jmap(stability, 0.08f, 0.001f);
    if (std::abs(x) < gateThreshold) return x * 0.1f;
    float positiveDrive = 1.8f;
    float negativeDrive = juce::jmap(stability, 0.7f, 1.3f);
    if (x > 0)
        return (1.0f - std::exp(-x * positiveDrive)) * 0.85f;
    else
        return (-1.0f + std::exp(std::abs(x) * negativeDrive)) * 0.85f;
}

void DistortionProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&) {
    juce::ScopedNoDenormals noDenormals;

    // === FIX P1: Add standard boilerplate ===
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();

    // Clear any output channels that didn't contain input data
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());
    // ========================================

    auto type = static_cast<Algo>(static_cast<int>(mainApvts.getRawParameterValue(typeParamId)->load()));
    preGain.setGainDecibels(mainApvts.getRawParameterValue(driveParamId)->load());
    postGain.setGainDecibels(mainApvts.getRawParameterValue(levelParamId)->load());
    smoothedBias.setTargetValue(mainApvts.getRawParameterValue(biasParamId)->load());
    smoothedCharacter.setTargetValue(mainApvts.getRawParameterValue(characterParamId)->load());

    juce::dsp::AudioBlock<float> block(buffer);
    juce::dsp::ProcessContextReplacing<float> context(block);

    inputDCBlocker.process(context);
    preGain.process(context);

    auto oversampledBlock = localOversampler->processSamplesUp(block);

    float currentBias = 0.0f;
    float currentCharacter = 0.0f;

    // === FIX (C4267): Explicitly cast dimensions to int and use int for loop indices ===
    int numOversampledSamples = (int)oversampledBlock.getNumSamples();
    int numChannels = (int)oversampledBlock.getNumChannels();

    for (int i = 0; i < numOversampledSamples; ++i)
    {
        currentBias = smoothedBias.getNextValue();
        currentCharacter = smoothedCharacter.getNextValue();
        float currentDynamicBias = 0.0f;

        for (int channel = 0; channel < numChannels; ++channel)
        {
            // Arguments are now int, resolving warnings
            float inputSample = oversampledBlock.getSample(channel, i);

            if (channel == 0)
                currentDynamicBias = inputFollower.process(inputSample);

            float outputSample = inputSample;
            switch (type)
            {
            case Algo::VintageTube:
                outputSample = processTube(inputSample, currentBias, currentDynamicBias);
                break;
            case Algo::OpAmp:
                outputSample = processOpAmp(inputSample, currentCharacter);
                break;
            case Algo::GermaniumFuzz:
                outputSample = processGermanium(inputSample, currentCharacter);
                break;
            }
            // Arguments are now int, resolving warnings
            oversampledBlock.setSample(channel, i, outputSample);
        }
    }
    // =====================================================================================

    localOversampler->processSamplesDown(block);
    outputDCBlocker.process(context);
    postGain.process(context);
}

================================================================================
// File: DistortionProcessor.h
================================================================================

﻿#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>
#include "../DSPUtils.h"

class DistortionProcessor : public juce::AudioProcessor
{
public:
    DistortionProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~DistortionProcessor() override = default;

    const juce::String getName() const override { return "Distortion"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    double getTailLengthSeconds() const override { return 0.0; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

private:
    enum class Algo { VintageTube, OpAmp, GermaniumFuzz };

    juce::dsp::Gain<float> preGain;
    juce::dsp::Gain<float> postGain;

    // === FIX (C2039): Replace DCBlocker with ProcessorDuplicator for robust multi-channel filtering ===
    using DCFilter = juce::dsp::IIR::Filter<float>;
    using DCFilterState = juce::dsp::IIR::Coefficients<float>;
    juce::dsp::ProcessorDuplicator<DCFilter, DCFilterState> inputDCBlocker;
    juce::dsp::ProcessorDuplicator<DCFilter, DCFilterState> outputDCBlocker;
    // ===================================================================================================

    std::unique_ptr<juce::dsp::Oversampling<float>> localOversampler;
    DSPUtils::EnvelopeFollower inputFollower;

    float processTube(float x, float bias, float dynamicBias);
    float processOpAmp(float x, float character);
    float processGermanium(float x, float stability);

    juce::AudioProcessorValueTreeState& mainApvts;
    juce::String driveParamId, levelParamId, typeParamId, biasParamId, characterParamId;
    // Smoothed values for parameters not handled by dsp::Gain
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedBias;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedCharacter;
};

================================================================================
// File: FilterProcessor.cpp
================================================================================

#include "FilterProcessor.h"

FilterProcessor::FilterProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_";
    cutoffParamId = slotPrefix + "FILTER_CUTOFF";
    resonanceParamId = slotPrefix + "FILTER_RESONANCE";
    driveParamId = slotPrefix + "FILTER_DRIVE";
    typeParamId = slotPrefix + "FILTER_TYPE";
    profileParamId = slotPrefix + "FILTER_PROFILE";
}

void FilterProcessor::prepareToPlay(double sampleRate, int samplesPerBlock) {
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };
    svfFilter.prepare(spec);
    ladderFilter.prepare(spec);
    reset();
}

void FilterProcessor::releaseResources() {
    reset();
}

void FilterProcessor::reset() {
    svfFilter.reset();
    ladderFilter.reset();
}

void FilterProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&) {
    juce::ScopedNoDenormals noDenormals;

    // === FIX P1: Add standard boilerplate ===
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();

    // Clear any output channels that didn't contain input data
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());
    // ========================================

    auto profile = static_cast<Profile>(static_cast<int>(mainApvts.getRawParameterValue(profileParamId)->load()));
    juce::dsp::AudioBlock<float> block(buffer);
    juce::dsp::ProcessContextReplacing<float> context(block);

    float rawResonance = mainApvts.getRawParameterValue(resonanceParamId)->load();

    switch (profile)
    {
    case svfProfile:
        svfFilter.setCutoffFrequency(mainApvts.getRawParameterValue(cutoffParamId)->load());
        svfFilter.setResonance(rawResonance);
        svfFilter.setType(static_cast<juce::dsp::StateVariableTPTFilterType>(static_cast<int>(mainApvts.getRawParameterValue(typeParamId)->load())));
        svfFilter.process(context);
        break;

    case transistorLadder:
    {
        ladderFilter.setMode(juce::dsp::LadderFilterMode::LPF24);
        ladderFilter.setCutoffFrequencyHz(mainApvts.getRawParameterValue(cutoffParamId)->load());
        float ladderResonance = juce::jlimit(0.0f, 1.0f, rawResonance / 10.0f);
        ladderFilter.setResonance(ladderResonance);
        ladderFilter.setDrive(mainApvts.getRawParameterValue(driveParamId)->load());
        ladderFilter.process(context);
        break;
    }
    case diodeLadder:
    {
        ladderFilter.setMode(juce::dsp::LadderFilterMode::LPF12);
        ladderFilter.setCutoffFrequencyHz(mainApvts.getRawParameterValue(cutoffParamId)->load());
        float ladderResonanceDiode = juce::jlimit(0.0f, 1.0f, rawResonance / 10.0f);
        ladderFilter.setResonance(ladderResonanceDiode);
        ladderFilter.setDrive(mainApvts.getRawParameterValue(driveParamId)->load());
        ladderFilter.process(context);
        break;
    }
    case ota:
        svfFilter.setType(juce::dsp::StateVariableTPTFilterType::lowpass);
        svfFilter.setCutoffFrequency(mainApvts.getRawParameterValue(cutoffParamId)->load());
        svfFilter.setResonance(rawResonance);
        svfFilter.process(context);
        break;
    }
}

================================================================================
// File: FilterProcessor.h
================================================================================

#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>

class FilterProcessor : public juce::AudioProcessor
{
public:
    FilterProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~FilterProcessor() override = default;

    const juce::String getName() const override { return "Filter"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    double getTailLengthSeconds() const override { return 0.0; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

    enum Profile { svfProfile, transistorLadder, diodeLadder, ota };

private:
    juce::dsp::StateVariableTPTFilter<float> svfFilter;
    juce::dsp::LadderFilter<float> ladderFilter;

    juce::AudioProcessorValueTreeState& mainApvts;
    juce::String cutoffParamId, resonanceParamId, driveParamId, typeParamId, profileParamId;
};

================================================================================
// File: ModulationProcessor.cpp
================================================================================

#include "ModulationProcessor.h"

ModulationProcessor::ModulationProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_";
    modeParamId = slotPrefix + "MODULATION_MODE";
    rateParamId = slotPrefix + "MODULATION_RATE";
    depthParamId = slotPrefix + "MODULATION_DEPTH";
    feedbackParamId = slotPrefix + "MODULATION_FEEDBACK";
    mixParamId = slotPrefix + "MODULATION_MIX";
}

void ModulationProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };
    chorus.prepare(spec);
    phaser.prepare(spec);
    reset();
}

void ModulationProcessor::releaseResources()
{
    reset();
}

void ModulationProcessor::reset()
{
    chorus.reset();
    phaser.reset();
}

void ModulationProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&)
{
    juce::ScopedNoDenormals noDenormals;

    // === FIX P1: Add standard boilerplate ===
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();

    // Clear any output channels that didn't contain input data
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());
    // ========================================

    auto mode = static_cast<ModType>(static_cast<int>(mainApvts.getRawParameterValue(modeParamId)->load()));
    float rate = mainApvts.getRawParameterValue(rateParamId)->load();
    float depth = mainApvts.getRawParameterValue(depthParamId)->load();
    float feedback = mainApvts.getRawParameterValue(feedbackParamId)->load();
    float mix = mainApvts.getRawParameterValue(mixParamId)->load();

    juce::dsp::AudioBlock<float> block(buffer);
    juce::dsp::ProcessContextReplacing<float> context(block);

    if (mode == Phaser)
    {
        phaser.setRate(rate);
        phaser.setDepth(depth);
        phaser.setFeedback(feedback);
        phaser.setMix(mix);
        phaser.process(context);
    }
    else
    {
        chorus.setRate(rate);
        chorus.setDepth(depth);
        chorus.setFeedback(feedback);
        chorus.setMix(mode == Vibrato ? 1.0f : mix);
        chorus.setCentreDelay(mode == Flanger ? 2.0f : 10.0f);
        chorus.process(context);
    }
}

================================================================================
// File: ModulationProcessor.h
================================================================================

#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>

class ModulationProcessor : public juce::AudioProcessor
{
public:
    ModulationProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~ModulationProcessor() override = default;

    const juce::String getName() const override { return "Modulation"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    double getTailLengthSeconds() const override { return 0.0; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

private:
    juce::dsp::Chorus<float> chorus;
    juce::dsp::Phaser<float> phaser;

    enum ModType { Chorus, Flanger, Vibrato, Phaser };

    juce::AudioProcessorValueTreeState& mainApvts;
    juce::String modeParamId, rateParamId, depthParamId, feedbackParamId, mixParamId;
};

================================================================================
// File: MorphoCompProcessor.cpp
================================================================================

#include "MorphoCompProcessor.h"
// REMOVED: Internal SignalAnalyzer implementation.

MorphoCompProcessor::MorphoCompProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_";
    amountParamId = slotPrefix + "MORPHO_AMOUNT";
    responseParamId = slotPrefix + "MORPHO_RESPONSE";
    modeParamId = slotPrefix + "MORPHO_MODE";
    morphXParamId = slotPrefix + "MORPHO_X";
    morphYParamId = slotPrefix + "MORPHO_Y";
    mixParamId = slotPrefix + "MORPHO_MIX";
}

void MorphoCompProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };

    // UPDATED: Prepare the new analyzers
    spectralAnalyzer.prepare(spec);
    transientDetector.prepare(spec);

    compressor.prepare(spec);
    saturator.prepare(spec);

    // Initial smoothing time (will be updated dynamically in processBlock)
    morphXSmoother.reset(sampleRate, 0.1);
    morphYSmoother.reset(sampleRate, 0.1);

    // NEW: Report latency introduced by the analyzers.
    // The latency is determined by the hop size of the analyzers (e.g., 256 samples).
    setLatencySamples(transientDetector.getLatencyInSamples());

    reset();
}

void MorphoCompProcessor::releaseResources() {
    reset();
}

void MorphoCompProcessor::reset() {
    // UPDATED: Reset the new analyzers
    spectralAnalyzer.reset();
    transientDetector.reset();

    compressor.reset();
    saturator.reset();
    morphXSmoother.setCurrentAndTargetValue(0.5f);
    morphYSmoother.setCurrentAndTargetValue(0.5f);
}

template <typename T>
T bilinearInterp(T c00, T c10, T c01, T c11, float tx, float ty)
{
    T a = c00 * (1.0f - tx) + c10 * tx;
    T b = c01 * (1.0f - tx) + c11 * tx;
    return a * (1.0f - ty) + b * ty;
}

void MorphoCompProcessor::updateCompressorAndSaturation(float amount, float response, float morphX, float morphY)
{
    float attackFactor = bilinearInterp(Topologies::VCA.attackFactor, Topologies::FET.attackFactor,
        Topologies::Opto.attackFactor, Topologies::VariMu.attackFactor, morphX, morphY);
    float releaseFactor = bilinearInterp(Topologies::VCA.releaseFactor, Topologies::FET.releaseFactor,
        Topologies::Opto.releaseFactor, Topologies::VariMu.releaseFactor, morphX, morphY);
    float ratioFactor = bilinearInterp(Topologies::VCA.ratioFactor, Topologies::FET.ratioFactor,
        Topologies::Opto.ratioFactor, Topologies::VariMu.ratioFactor, morphX, morphY);
    float saturationDrive = bilinearInterp(Topologies::VCA.saturationDrive, Topologies::FET.saturationDrive,
        Topologies::Opto.saturationDrive, Topologies::VariMu.saturationDrive, morphX, morphY);

    float baseThreshold = juce::jmap(amount, 0.0f, -40.0f);
    float baseRatio = juce::jmap(amount, 1.5f, 8.0f);

    float baseAttack = std::pow(10.0f, juce::jmap(response, 2.0f, 0.0f));
    float baseRelease = std::pow(10.0f, juce::jmap(response, 3.0f, 1.5f));

    compressor.setThreshold(baseThreshold);
    compressor.setRatio(juce::jlimit(1.0f, 20.0f, baseRatio * ratioFactor));
    compressor.setAttack(juce::jlimit(0.1f, 500.0f, baseAttack * attackFactor));
    compressor.setRelease(juce::jlimit(5.0f, 2000.0f, baseRelease * releaseFactor));

    currentSaturationDrive = 1.0f + saturationDrive;

    if (morphX > 0.5f && morphY < 0.5f) saturator.functionToUse = Topologies::FET.saturationFunc;
    else if (morphX < 0.5f && morphY > 0.5f) saturator.functionToUse = Topologies::Opto.saturationFunc;
    else if (morphX > 0.5f && morphY > 0.5f) saturator.functionToUse = Topologies::VariMu.saturationFunc;
    else saturator.functionToUse = Topologies::VCA.saturationFunc;
}

void MorphoCompProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&) {
    juce::ScopedNoDenormals noDenormals;

    // P1 Boilerplate
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());

    int numSamples = buffer.getNumSamples();
    int numChannels = totalNumInputChannels;

    // 1. Run Analysis (Sample-by-sample)
    // We must run the analysis sample-by-sample to update the analyzer states correctly over time.
    for (int i = 0; i < numSamples; ++i)
    {
        // Calculate mono sum
        float monoSample = 0.0f;
        for (int ch = 0; ch < numChannels; ++ch)
        {
            monoSample += buffer.getSample(ch, i);
        }
        if (numChannels > 0)
            monoSample /= (float)numChannels;

        // Process analysis using the refactored sample-based interface
        spectralAnalyzer.processSample(monoSample);
        transientDetector.processSample(monoSample);
    }


    // 2. Get Parameters and Analysis Results
    float amount = mainApvts.getRawParameterValue(amountParamId)->load();
    float response = mainApvts.getRawParameterValue(responseParamId)->load();
    bool autoMorph = mainApvts.getRawParameterValue(modeParamId)->load() > 0.5f; // 0=Auto, 1=Manual - Corrected logic
    float mix = mainApvts.getRawParameterValue(mixParamId)->load();

    float targetX, targetY;
    if (autoMorph)
    {
        // Get analysis results. Since we ran them through the whole block,
        // these return the state at the END of the block (which is fine for setting the target).
        targetX = transientDetector.getTransientValue();
        // Invert centroid (Low centroid = High Y, more 'Opto/VariMu')
        targetY = 1.0f - spectralAnalyzer.getSpectralCentroid();
    }
    else
    {
        targetX = mainApvts.getRawParameterValue(morphXParamId)->load();
        targetY = mainApvts.getRawParameterValue(morphYParamId)->load();
    }

    // 3. Configure Smoothing
    // FIX: Dynamic Smoothing Time
    // Auto mode needs moderate smoothing (200ms), Manual mode needs fast smoothing (50ms).
    double smoothingTime = autoMorph ? 0.2 : 0.05;

    // Update the smoother's ramp length dynamically.
    if (getSampleRate() > 0)
    {
        morphXSmoother.reset(getSampleRate(), smoothingTime);
        morphYSmoother.reset(getSampleRate(), smoothingTime);
    }

    morphXSmoother.setTargetValue(targetX);
    morphYSmoother.setTargetValue(targetY);

    // 4. Synchronize Smoothers and Update Parameters
    // FIX: Correct Smoother Synchronization (Required for block processing)
    // Advance the smoother once to get the starting value for the block.
    float currentX = morphXSmoother.getNextValue();
    float currentY = morphYSmoother.getNextValue();

    // Skip the remaining samples in the block to keep the smoother synchronized.
    if (numSamples > 1)
    {
        morphXSmoother.skip(numSamples - 1);
        morphYSmoother.skip(numSamples - 1);
    }

    // Update compressor parameters based on the smoothed position
    updateCompressorAndSaturation(amount, response, currentX, currentY);

    // 5. Audio Processing (Block-wise)
    juce::dsp::AudioBlock<float> block(buffer);

    juce::AudioBuffer<float> dryBuffer;
    if (mix < 1.0f) dryBuffer.makeCopyOf(buffer);

    juce::dsp::ProcessContextReplacing<float> context(block);
    compressor.process(context);

    if (currentSaturationDrive > 1.01f)
    {
        block.multiplyBy(currentSaturationDrive);
        saturator.process(context);
        block.multiplyBy(1.0f / currentSaturationDrive);
    }

    if (mix < 1.0f)
    {
        for (int ch = 0; ch < buffer.getNumChannels(); ++ch)
        {
            if (ch < dryBuffer.getNumChannels())
            {
                buffer.addFrom(ch, 0, dryBuffer, ch, 0, numSamples, 1.0f - mix);
                buffer.applyGain(ch, 0, numSamples, mix);
            }
        }
    }
}


================================================================================
// File: MorphoCompProcessor.h
================================================================================

#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>
#include "../DSPUtils.h"
// NEW: Include the robust analysis helpers
#include "../DSP_Helpers/SpectralAnalyzer.h"
#include "../DSP_Helpers/TransientDetector.h"

// REMOVED: Internal flawed SignalAnalyzer class definition.

// [TopologyParams struct and Topologies namespace - Identical to input, omitted for brevity]
struct TopologyParams
{
    float attackFactor;
    float releaseFactor;
    float ratioFactor;
    float saturationDrive;
    float (*saturationFunc)(float);
};

namespace Topologies
{
    static float vcaSaturation(float x) { return std::tanh(x); }
    static float fetSaturation(float x) { return x / (std::abs(x) + 0.7f); }
    static float optoSaturation(float x) { return std::tanh(x * 0.8f); }
    static float varimuSaturation(float x) { return std::tanh(x * 1.5f); }

    const TopologyParams VCA = { 1.0f, 1.0f, 1.0f, 0.5f, vcaSaturation };
    const TopologyParams FET = { 0.2f, 0.8f, 1.5f, 1.5f, fetSaturation };
    const TopologyParams Opto = { 2.0f, 1.5f, 0.8f, 0.2f, optoSaturation };
    const TopologyParams VariMu = { 1.5f, 2.0f, 0.9f, 1.0f, varimuSaturation };
}


class MorphoCompProcessor : public juce::AudioProcessor
{
public:
    MorphoCompProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~MorphoCompProcessor() override = default;

    const juce::String getName() const override { return "MorphoComp"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    // [JUCE Boilerplate - Identical to input]
    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    // Increased tail length slightly due to analysis latency and release times.
    double getTailLengthSeconds() const override { return 0.5; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

private:
    void updateCompressorAndSaturation(float amount, float response, float morphX, float morphY);

    // UPDATED: Use the robust analysis helpers instead of the internal analyzer
    // SignalAnalyzer analyzer; // REMOVED
    SpectralAnalyzer spectralAnalyzer;
    TransientDetector transientDetector;

    juce::dsp::Compressor<float> compressor;
    juce::dsp::WaveShaper<float> saturator;

    // Use Linear smoothing for the control signals (X/Y)
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> morphXSmoother, morphYSmoother;
    float currentSaturationDrive = 1.0f;

    juce::AudioProcessorValueTreeState& mainApvts;
    juce::String amountParamId, responseParamId, modeParamId, morphXParamId, morphYParamId, mixParamId;
};


================================================================================
// File: PhysicalResonatorProcessor.cpp
================================================================================

﻿//================================================================================
// File: FX_Modules/PhysicalResonatorProcessor.cpp
//================================================================================
#include "PhysicalResonatorProcessor.h"

// =============================================================================
// ExcitationGenerator Implementation
// =============================================================================

ExcitationGenerator::ExcitationGenerator()
{
    noiseGen.setType(DSPUtils::NoiseGenerator::NoiseType::White);
}

void ExcitationGenerator::prepare(const juce::dsp::ProcessSpec& spec) {
    sampleRate = spec.sampleRate;
    transientDetector.prepare(spec);
    spectralAnalyzer.prepare(spec);

    // Configure color filter (Stereo)
    colorFilter.prepare(spec);
    // Using Bandpass filtering often yields more focused excitation
    colorFilter.setType(juce::dsp::StateVariableTPTFilterType::bandpass);

    burstEnvelope.setSampleRate(sampleRate);

    // FIX: Initialize smoothers (50ms)
    smoothedExciteType.reset(sampleRate, 0.05);
    smoothedSensitivity.reset(sampleRate, 0.05);

    reset();
}

void ExcitationGenerator::reset() {
    transientDetector.reset();
    spectralAnalyzer.reset();
    colorFilter.reset();
    burstEnvelope.reset();
    // FIX: Reset smoothers
    smoothedExciteType.setCurrentAndTargetValue(0.5f);
    smoothedSensitivity.setCurrentAndTargetValue(0.5f);
}

// FIX: Updated process loop for time-aligned analysis and smoothed parameters.
void ExcitationGenerator::process(const juce::dsp::AudioBlock<float>& inputBlock, juce::dsp::AudioBlock<float>& outputExcitationBlock, const ExcitationParams& params) {
    // 1. Set Noise Type
    noiseGen.setType(params.noiseType == 1 ? DSPUtils::NoiseGenerator::NoiseType::Pink : DSPUtils::NoiseGenerator::NoiseType::White);

    // 2. Update ADSR Parameters (Block-wise update is fine)
    juce::ADSR::Parameters adsrParams;
    adsrParams.attack = params.attack;
    adsrParams.decay = params.decay;
    adsrParams.sustain = params.sustain;
    adsrParams.release = params.release;
    burstEnvelope.setParameters(adsrParams);

    // FIX: Update Smoother Targets
    smoothedExciteType.setTargetValue(params.exciteType);
    smoothedSensitivity.setTargetValue(params.sensitivity);

    // Rely on the inputBlock dimensions.
    int numSamples = (int)inputBlock.getNumSamples();
    int numChannels = (int)inputBlock.getNumChannels();

    for (int i = 0; i < numSamples; ++i)
    {
        // --- Analysis (Per-Sample) ---

        // Calculate mono sum for analysis
        float monoSample = 0.0f;
        for (int ch = 0; ch < numChannels; ++ch)
        {
            monoSample += inputBlock.getSample(ch, i);
        }
        if (numChannels > 0)
            monoSample /= (float)numChannels;

        // Analyze input features sample-by-sample using the refactored interface
        transientDetector.processSample(monoSample);
        spectralAnalyzer.processSample(monoSample);

        // Get smoothed parameters for this sample
        float currentExciteType = smoothedExciteType.getNextValue();
        float currentSensitivity = smoothedSensitivity.getNextValue();

        // Use spectral centroid to adaptively color the noise burst
        float centroid = spectralAnalyzer.getSpectralCentroid();
        float cutoff = juce::jmap(centroid, 500.0f, 10000.0f);
        // Update filter cutoff (TPT filters handle smooth updates internally)
        colorFilter.setCutoffFrequency(cutoff);

        // Define trigger threshold based on smoothed sensitivity
        float triggerThreshold = juce::jmap(currentSensitivity, 0.8f, 0.2f);
        float transientLevel = transientDetector.getTransientValue();


        // Triggering logic (Blueprint 3.2)
        if (transientLevel > triggerThreshold && !burstEnvelope.isActive())
            burstEnvelope.noteOn();

        // Handle noteOff for sustained sounds (using original params.sustain for duration check)
        if (params.sustain > 0.001f && transientLevel < (triggerThreshold * 0.5f) && burstEnvelope.isActive())
        {
            burstEnvelope.noteOff();
        }

        float envelopeValue = burstEnvelope.getNextSample();

        // --- Synthesis (Per-Sample, Per-Channel) ---
        for (int ch = 0; ch < numChannels; ++ch)
        {
            // Source A: Continuous Input (using smoothed sensitivity)
            float continuous = inputBlock.getSample(ch, i) * currentSensitivity;

            // Source B: Impulsive Noise Burst
            float noise = noiseGen.getNextSample();
            // Apply filtering to the noise burst (Stereo filter)
            float filteredNoise = colorFilter.processSample(ch, noise);
            float impulsive = filteredNoise * envelopeValue;

            // Crossfade (using smoothed exciteType)
            float excitation = (continuous * (1.0f - currentExciteType)) + (impulsive * currentExciteType);

            // Ensure output block matches input block channel count (Safety assertion)
            jassert(ch < (int)outputExcitationBlock.getNumChannels());
            outputExcitationBlock.setSample(ch, i, excitation);
        }
    }
}

// =============================================================================
// ModalResonator Implementation
// [Identical to input, omitted for brevity]
// =============================================================================

// =============================================================================
// PhysicalResonatorProcessor Implementation
// =============================================================================
PhysicalResonatorProcessor::PhysicalResonatorProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_PHYSRES_";
    modelParamId = slotPrefix + "MODEL";
    tuneParamId = slotPrefix + "TUNE";
    structureParamId = slotPrefix + "STRUCTURE";
    brightnessParamId = slotPrefix + "BRIGHTNESS";
    dampingParamId = slotPrefix + "DAMPING";
    positionParamId = slotPrefix + "POSITION";
    exciteTypeParamId = slotPrefix + "EXCITE_TYPE";
    sensitivityParamId = slotPrefix + "SENSITIVITY";
    mixParamId = slotPrefix + "MIX";
    noiseTypeParamId = slotPrefix + "NOISE_TYPE";
    attackParamId = slotPrefix + "ATTACK";
    decayParamId = slotPrefix + "DECAY";
    sustainParamId = slotPrefix + "SUSTAIN";
    releaseParamId = slotPrefix + "RELEASE";
}

void PhysicalResonatorProcessor::prepareToPlay(double sampleRate, int samplesPerBlock) {
    // Configure spec based on input channels
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };

    // Initialize components
    excitationGenerator.prepare(spec);

    // Prepare all resonator instances
    modalResonator.prepare(spec);
    sympatheticResonator.prepare(spec);
    stringResonator.prepare(spec);

    // Ensure buffers are correctly sized based on input channels
    excitationBuffer.setSize((int)spec.numChannels, (int)spec.maximumBlockSize);
    wetOutputBuffer.setSize((int)spec.numChannels, (int)spec.maximumBlockSize);

    // Initialize smoothers
    double smoothingTime = 0.05; // 50ms smoothing
    smoothedTune.reset(sampleRate, smoothingTime);
    smoothedStructure.reset(sampleRate, smoothingTime);
    smoothedBrightness.reset(sampleRate, smoothingTime);
    smoothedDamping.reset(sampleRate, smoothingTime);
    smoothedPosition.reset(sampleRate, smoothingTime);
    // FIX: Removed ExciteType and Sensitivity smoothers (now in ExcitationGenerator)
    smoothedMix.reset(sampleRate, smoothingTime);

    reset();
}

void PhysicalResonatorProcessor::releaseResources() { reset(); }
void PhysicalResonatorProcessor::reset() {
    excitationGenerator.reset();
    modalResonator.reset();
    sympatheticResonator.reset();
    stringResonator.reset();
}

void PhysicalResonatorProcessor::updateResonatorCore(int newModelIndex) {
    // ... implementation
}

void PhysicalResonatorProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&) {
    // [Boilerplate and setup - Identical to input]
    juce::ScopedNoDenormals noDenormals;
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());
    int numSamples = buffer.getNumSamples();
    int numChannels = totalNumInputChannels;
    if (numChannels == 0) return;


    // 1. Update Model if necessary
    int modelIndex = static_cast<int>(mainApvts.getRawParameterValue(modelParamId)->load());
    updateResonatorCore(modelIndex);

    if (activeResonator == nullptr) return;

    // 2. Update Smoothers (Targets set once per block)
    smoothedTune.setTargetValue(mainApvts.getRawParameterValue(tuneParamId)->load());
    smoothedStructure.setTargetValue(mainApvts.getRawParameterValue(structureParamId)->load());
    smoothedBrightness.setTargetValue(mainApvts.getRawParameterValue(brightnessParamId)->load());
    smoothedDamping.setTargetValue(mainApvts.getRawParameterValue(dampingParamId)->load());
    smoothedPosition.setTargetValue(mainApvts.getRawParameterValue(positionParamId)->load());
    // FIX: Removed ExciteType and Sensitivity smoother updates
    smoothedMix.setTargetValue(mainApvts.getRawParameterValue(mixParamId)->load());

    // [Buffer Management and AudioBlock definitions]
    if (excitationBuffer.getNumSamples() < numSamples || excitationBuffer.getNumChannels() < numChannels)
    {
        excitationBuffer.setSize(numChannels, numSamples, false, true, true);
        wetOutputBuffer.setSize(numChannels, numSamples, false, true, true);
    }
    juce::dsp::AudioBlock<float> dryBlock(buffer);
    juce::dsp::AudioBlock<float> excitationBlock(excitationBuffer);
    juce::dsp::AudioBlock<float> wetBlock(wetOutputBuffer);
    auto activeDryBlock = dryBlock.getSubBlock(0, (size_t)numSamples).getSubsetChannelBlock(0, (size_t)numChannels);
    auto activeExcitationBlock = excitationBlock.getSubBlock(0, (size_t)numSamples);
    auto activeWetBlock = wetBlock.getSubBlock(0, (size_t)numSamples);
    activeExcitationBlock.clear();
    activeWetBlock.clear();

    // 3. STAGE 1: Generate Excitation

    // NEW: Gather parameters for the Excitation Generator (Read directly from APVTS)
    // The generator now handles the smoothing internally.
    ExcitationGenerator::ExcitationParams exciteParams;
    exciteParams.exciteType = mainApvts.getRawParameterValue(exciteTypeParamId)->load();
    exciteParams.sensitivity = mainApvts.getRawParameterValue(sensitivityParamId)->load();
    exciteParams.noiseType = static_cast<int>(mainApvts.getRawParameterValue(noiseTypeParamId)->load());
    exciteParams.attack = mainApvts.getRawParameterValue(attackParamId)->load();
    exciteParams.decay = mainApvts.getRawParameterValue(decayParamId)->load();
    exciteParams.sustain = mainApvts.getRawParameterValue(sustainParamId)->load();
    exciteParams.release = mainApvts.getRawParameterValue(releaseParamId)->load();

    // Generate excitation
    excitationGenerator.process(activeDryBlock, activeExcitationBlock, exciteParams);

    // 4. STAGE 2: Process Resonator Core
    for (int i = 0; i < numSamples; ++i)
    {
        float tune = smoothedTune.getNextValue();
        float structure = smoothedStructure.getNextValue();
        float brightness = smoothedBrightness.getNextValue();
        float damping = smoothedDamping.getNextValue();
        float position = smoothedPosition.getNextValue();
        auto excitationSampleBlock = activeExcitationBlock.getSubBlock((size_t)i, 1);
        auto wetOutputSampleBlock = activeWetBlock.getSubBlock((size_t)i, 1);
        activeResonator->process(excitationSampleBlock, wetOutputSampleBlock, tune, structure, brightness, damping, position);
    }

    // 5. STAGE 3: Final Mix
    for (int i = 0; i < numSamples; ++i)
    {
        float mix = smoothedMix.getNextValue();
        for (int ch = 0; ch < totalNumOutputChannels; ++ch)
        {
            float dry = (ch < numChannels) ? buffer.getSample(ch, i) : 0.0f;
            float wet = (ch < numChannels) ? activeWetBlock.getSample(ch, i) : 0.0f;
            buffer.setSample(ch, i, (dry * (1.0f - mix)) + (wet * mix));
        }
    }
}


================================================================================
// File: PhysicalResonatorProcessor.h
================================================================================

//================================================================================
// File: FX_Modules/PhysicalResonatorProcessor.h
//================================================================================
#pragma once

#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>
#include <array>
#include <cmath>
#include <algorithm>

// Include necessary DSP helpers.
#include "../DSPUtils.h"
#include "../DSP_Helpers/TransientDetector.h"
#include "../DSP_Helpers/SpectralAnalyzer.h"

// =============================================================================
// Helper Class: ExcitationGenerator (Blueprint 3.2)
// =============================================================================
class ExcitationGenerator
{
public:
    ExcitationGenerator();
    void prepare(const juce::dsp::ProcessSpec& spec);
    void reset();

    // NEW: Structure to hold parameters
    struct ExcitationParams
    {
        float exciteType = 0.5f;
        float sensitivity = 0.5f;
        int noiseType = 0; // 0: White, 1: Pink
        float attack = 0.001f;
        float decay = 0.05f;
        float sustain = 0.0f;
        float release = 0.01f;
    };

    // Updated process signature
    void process(const juce::dsp::AudioBlock<float>& inputBlock, juce::dsp::AudioBlock<float>& outputExcitationBlock, const ExcitationParams& params);
private:
    double sampleRate = 44100.0;
    // Analysis components
    TransientDetector transientDetector;
    SpectralAnalyzer spectralAnalyzer;

    // Synthesis components
    DSPUtils::NoiseGenerator noiseGen;

    // Use a valid JUCE filter (StateVariableTPTFilter) for color shaping.
    juce::dsp::StateVariableTPTFilter<float> colorFilter;

    // Envelope for impulsive bursts
    juce::ADSR burstEnvelope;

    // FIX: Add declarations for the smoothed values used internally by the generator.
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedExciteType;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedSensitivity;
};

// =============================================================================
// Resonator Cores (Abstract Base and Implementations) (Blueprint 3.1)
// =============================================================================

class ResonatorCore
{
public:
    virtual ~ResonatorCore() = default;
    virtual void prepare(const juce::dsp::ProcessSpec& spec) = 0;
    virtual void reset() = 0;
    // Parameters are passed during process() to allow for per-sample updates.
    virtual void process(const juce::dsp::AudioBlock<float>& excitationBlock, juce::dsp::AudioBlock<float>& outputBlock,
        float tuneHz, float structure, float brightness, float damping, float position) = 0;
};

// -----------------------------------------------------------------------------
// Core I: ModalResonator (Blueprint 3.3)
// -----------------------------------------------------------------------------

class ModalResonator : public ResonatorCore
{
public:
    static constexpr int NUM_MODES = 16;
    // Structure to hold material data (Blueprint Table 3.3.1)
    struct MaterialData {
        std::array<float, NUM_MODES> ratios;
        std::array<float, NUM_MODES> gains;
        std::array<float, NUM_MODES> qs;
    };

    // Static material definitions (Initialized in .cpp)
    static const MaterialData woodData;
    static const MaterialData metalData;
    static const MaterialData inharmonicData;

    void prepare(const juce::dsp::ProcessSpec& spec) override;
    void reset() override;
    // Renamed parameters to avoid shadowing warnings (C4458)
    void process(const juce::dsp::AudioBlock<float>& excitationBlock, juce::dsp::AudioBlock<float>& outputBlock,
        float pTuneHz, float pStructure, float pBrightness, float pDamping, float pPosition) override;
private:
    void updateModes(float tuneHz, float structure, float brightness, float damping, float position);

    // Define the filter banks.
    // Using a vector of arrays for multi-channel support.
    using FilterType = juce::dsp::StateVariableTPTFilter<float>;
    // Vector (Channels) of Array (Modes)
    std::vector<std::array<FilterType, NUM_MODES>> channelFilterBanks;

    double sampleRate = 44100.0;
    // Storage for calculated mode parameters (needed for Q-normalization)
    std::array<float, NUM_MODES> currentModeGains;
    std::array<float, NUM_MODES> currentModeQs;
};

// -----------------------------------------------------------------------------
// Core II & III: Placeholders (Sympathetic/Inharmonic)
// -----------------------------------------------------------------------------
// Placeholder implementations to allow the architecture to compile.
class SympatheticStringResonator : public ResonatorCore
{
public:
    void prepare(const juce::dsp::ProcessSpec& spec) override { juce::ignoreUnused(spec); }
    void reset() override {}
    void process(const juce::dsp::AudioBlock<float>& excitationBlock, juce::dsp::AudioBlock<float>& outputBlock,
        float pTuneHz, float pStructure, float pBrightness, float pDamping, float pPosition) override
    {
        // FIX: Silence C4100 (unreferenced parameter) warnings.
        juce::ignoreUnused(excitationBlock, pTuneHz, pStructure, pBrightness, pDamping, pPosition);

        // Placeholder implementation: Clear output.
        outputBlock.clear();
    }
};

class StringResonator : public ResonatorCore
{
public:
    void prepare(const juce::dsp::ProcessSpec& spec) override { juce::ignoreUnused(spec); }
    void reset() override {}
    void process(const juce::dsp::AudioBlock<float>& excitationBlock, juce::dsp::AudioBlock<float>& outputBlock,
        float pTuneHz, float pStructure, float pBrightness, float pDamping, float pPosition) override
    {
        // FIX: Silence C4100 (unreferenced parameter) warnings.
        juce::ignoreUnused(excitationBlock, pTuneHz, pStructure, pBrightness, pDamping, pPosition);

        // Placeholder implementation: Clear output.
        outputBlock.clear();
    }
};

// =============================================================================
// Main Processor Class
// =============================================================================

class PhysicalResonatorProcessor : public juce::AudioProcessor
{
public:
    PhysicalResonatorProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~PhysicalResonatorProcessor() override = default;

    const juce::String getName() const override { return "Physical Resonator"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    // Standard JUCE Boilerplate
    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    double getTailLengthSeconds() const override { return 5.0; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

private:
    void updateResonatorCore(int newModelIndex);

    // --- Components ---
    ExcitationGenerator excitationGenerator;

    // Store concrete instances and use a pointer to the active one.
    ModalResonator modalResonator;
    SympatheticStringResonator sympatheticResonator;
    StringResonator stringResonator;
    ResonatorCore* activeResonator = nullptr;

    // Buffers (Ensure separate buffers for excitation and wet output)
    juce::AudioBuffer<float> excitationBuffer;
    juce::AudioBuffer<float> wetOutputBuffer;

    // --- Parameters and Smoothing ---
    juce::AudioProcessorValueTreeState& mainApvts;
    // Declare all parameter ID strings used in the .cpp
    juce::String modelParamId, tuneParamId, structureParamId, brightnessParamId, dampingParamId, positionParamId, exciteTypeParamId, sensitivityParamId, mixParamId;
    // NEW: Advanced Excitation controls
    juce::String noiseTypeParamId, attackParamId, decayParamId, sustainParamId, releaseParamId;

    // Declare all smoothed values used in the .cpp
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedTune;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedStructure;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedBrightness;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedDamping;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedPosition;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedExciteType;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedSensitivity;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedMix;

    int currentModelIndex = -1;
};

================================================================================
// File: ReverbProcessor.cpp
================================================================================

#include "ReverbProcessor.h"

ReverbProcessor::ReverbProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_";
    roomSizeParamId = slotPrefix + "REVERB_ROOM_SIZE";
    dampingParamId = slotPrefix + "REVERB_DAMPING";
    mixParamId = slotPrefix + "REVERB_MIX";
    widthParamId = slotPrefix + "REVERB_WIDTH";
}

void ReverbProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };
    reverb.prepare(spec);
    reset();
}

void ReverbProcessor::releaseResources()
{
    reset();
}

void ReverbProcessor::reset()
{
    reverb.reset();
}

void ReverbProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&)
{
    juce::ScopedNoDenormals noDenormals;

    // === FIX P1: Add standard boilerplate ===
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();

    // Clear any output channels that didn't contain input data
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());
    // ========================================

    juce::dsp::Reverb::Parameters reverbParams;
    reverbParams.roomSize = mainApvts.getRawParameterValue(roomSizeParamId)->load();
    reverbParams.damping = mainApvts.getRawParameterValue(dampingParamId)->load();
    reverbParams.wetLevel = mainApvts.getRawParameterValue(mixParamId)->load();
    reverbParams.dryLevel = 1.0f - reverbParams.wetLevel;
    reverbParams.width = mainApvts.getRawParameterValue(widthParamId)->load();
    reverb.setParameters(reverbParams);

    juce::dsp::AudioBlock<float> block(buffer);
    juce::dsp::ProcessContextReplacing<float> context(block);
    reverb.process(context);
}

================================================================================
// File: ReverbProcessor.h
================================================================================

#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include <juce_dsp/juce_dsp.h>

class ReverbProcessor : public juce::AudioProcessor
{
public:
    ReverbProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~ReverbProcessor() override = default;

    const juce::String getName() const override { return "Reverb"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    double getTailLengthSeconds() const override { return 8.0; }

    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

private:
    juce::dsp::Reverb reverb;

    juce::AudioProcessorValueTreeState& mainApvts;
    juce::String roomSizeParamId, dampingParamId, mixParamId, widthParamId;
};

================================================================================
// File: SpectralAnimatorEngine.cpp
================================================================================

﻿//================================================================================
// File: FX_Modules/SpectralAnimatorEngine.cpp
//================================================================================
#include "SpectralAnimatorEngine.h"

SpectralAnimatorEngine::SpectralAnimatorEngine()
    : forwardFFT(FFT_ORDER),
    // Initialize Hann window (suitable for 75% overlap)
    window(FFT_SIZE, juce::dsp::WindowingFunction<float>::WindowingMethod::hann)
{
    harmonicMask.resize(NUM_BINS, 0.0f);
    formantMask.resize(NUM_BINS, 0.0f);
}

// Helper function for Vowel Space Interpolation (Formant Mode)
SpectralAnimatorEngine::FormantProfile SpectralAnimatorEngine::getVowel(float x, float y)
{
    // X-axis: F2 (Front/Back) -> High X = Front ('i'), Low X = Back ('u')
    // Y-axis: F1 (Open/Close) -> High Y = Open ('a'), Low Y = Close ('i'/'u')

    // Define corner vowels (F1, F2) - typical adult male approximations
    const FormantProfile i = { 270, 2290 }; // Close Front ("see")
    const FormantProfile u = { 300, 870 };  // Close Back ("boot")
    const FormantProfile a = { 730, 1090 }; // Open Mid/Back ("father")
    const FormantProfile ae = { 660, 1720 }; // Open Front ("had")

    auto interpolate = [](float v1, float v2, float t) { return v1 * (1.0f - t) + v2 * t; };

    // Bilinear interpolation
    float f1_close = interpolate(u.f1, i.f1, x);
    float f1_open = interpolate(a.f1, ae.f1, x);
    float f1 = interpolate(f1_close, f1_open, y);

    float f2_close = interpolate(u.f2, i.f2, x);
    float f2_open = interpolate(a.f2, ae.f2, x);
    float f2 = interpolate(f2_close, f2_open, y);

    return { f1, f2 };
}


void SpectralAnimatorEngine::prepare(const juce::dsp::ProcessSpec& spec)
{
    sampleRate = spec.sampleRate;
    numChannels = (int)spec.numChannels;

    // 1. Initialize Buffers
    inputFIFO.setSize(numChannels, FFT_SIZE);
    // Output Buffer size: 2 * FFT_SIZE (provides ample space for OLA accumulation)
    int outputBufferSize = FFT_SIZE * 2;
    outputBuffer.setSize(numChannels, outputBufferSize);

    // Initialize temporary FFT buffers
    channelTimeDomain.resize(numChannels);
    channelFreqDomain.resize(numChannels);
    for (int ch = 0; ch < numChannels; ++ch)
    {
        channelTimeDomain[ch].resize(FFT_SIZE, 0.0f);
        // JUCE requires 2*N space for real-only FFT operations
        channelFreqDomain[ch].resize(FFT_SIZE * 2, 0.0f);
    }

    // 2. Initialize Transient Detectors (Per Channel)
    transientDetectors.resize(numChannels);
    juce::dsp::ProcessSpec monoSpec = spec;
    monoSpec.numChannels = 1;

    // 30ms decay for the mix envelope (as per design document)
    float decayTimeMs = 30.0f;

    for (auto& detector : transientDetectors)
    {
        detector.highPassFilter.prepare(monoSpec);
        // Set high-pass cutoff (e.g., 2kHz)
        detector.highPassFilter.setCutoffFrequency(2000.0f);

        detector.envelopeFollower.prepare(monoSpec);
        // Fast attack/release for the detection signal itself
        detector.envelopeFollower.setAttackTime(1.0f);
        detector.envelopeFollower.setReleaseTime(10.0f);

        // Calculate time-based exponential decay factor for the mix control
        if (sampleRate > 0)
            detector.decayFactor = std::exp(-1.0f / (float)(sampleRate * decayTimeMs / 1000.0f));
    }

    // FIX: Initialize smoothers (e.g., 5ms smoothing time for fast response)
    double smoothingTime = 0.005;
    smoothedMorph.reset(sampleRate, smoothingTime);
    smoothedTransientPreservation.reset(sampleRate, smoothingTime);

    reset();
}

void SpectralAnimatorEngine::reset()
{
    inputFIFO.clear();
    outputBuffer.clear();
    fifoIndex = 0;
    outputBufferWritePos = 0;
    outputBufferReadPos = 0;

    for (auto& detector : transientDetectors)
    {
        detector.highPassFilter.reset();
        detector.envelopeFollower.reset();
        detector.transientMix = 0.0f;
    }

    // FIX: Reset smoothers to default values (1.0)
    smoothedMorph.setCurrentAndTargetValue(1.0f);
    smoothedTransientPreservation.setCurrentAndTargetValue(1.0f);

    masksNeedUpdate = true;
}

// Parameter setters (trigger mask updates if necessary)
void SpectralAnimatorEngine::setMode(Mode newMode) { if (currentMode != newMode) { currentMode = newMode; masksNeedUpdate = true; } }
void SpectralAnimatorEngine::setPitch(float newPitchHz) { if (pitchHz != newPitchHz) { pitchHz = newPitchHz; masksNeedUpdate = true; } }
void SpectralAnimatorEngine::setFormant(float x, float y) { formantXY = { x, y }; masksNeedUpdate = true; }
void SpectralAnimatorEngine::setMorph(float amount) { smoothedMorph.setTargetValue(amount); }
void SpectralAnimatorEngine::setTransientPreservation(float amount) { smoothedTransientPreservation.setTargetValue(amount); }


// Main process loop implementing STFT buffering and transient mixing
void SpectralAnimatorEngine::process(juce::AudioBuffer<float>& buffer)
{
    int numSamples = buffer.getNumSamples();
    int outputBufferSize = outputBuffer.getNumSamples();

    // Update masks if parameters changed
    if (masksNeedUpdate)
    {
        updateMasks();
        masksNeedUpdate = false;
    }

    // Main audio loop
    for (int i = 0; i < numSamples; ++i)
    {
        bool frameReady = false;

        // FIX (C4189): Advance smoothers once per sample frame.
        // We must advance the smoother, but we don't use the 'morph' value in this loop scope.
        smoothedMorph.getNextValue();
        float currentTransientPreservation = smoothedTransientPreservation.getNextValue();


        for (int ch = 0; ch < numChannels; ++ch)
        {
            float inputSample = buffer.getSample(ch, i);

            // --- Transient Detection Path (2.2 Logic) ---
            auto& detector = transientDetectors[ch];

            // Note: Filters/Followers prepared with monoSpec require channel index 0 when calling processSample
            float highPassed = detector.highPassFilter.processSample(0, inputSample);
            float envelope = detector.envelopeFollower.processSample(0, std::abs(highPassed));

            // Fast attack, exponential decay mix control (The "transient preservation envelope")
            if (envelope > transientThreshold)
                detector.transientMix = 1.0f; // Attack immediately
            else
                detector.transientMix *= detector.decayFactor; // Decay smoothly


            // --- STFT Path (2.2 Logic) ---

            // 1. Push input sample into FIFO
            inputFIFO.setSample(ch, fifoIndex, inputSample);

            // 3. Retrieve output sample from OLA buffer
            float outputSample = outputBuffer.getSample(ch, outputBufferReadPos);
            // Clear the sample we just read for the next accumulation
            outputBuffer.setSample(ch, outputBufferReadPos, 0.0f);

            // 4. Final Mix (Transient Integration)
            // FIX: Use the smoothed value (currentTransientPreservation)
            float mixControl = detector.transientMix * currentTransientPreservation;
            // Linear crossfade: Wet * (1-Mix) + Dry * Mix
            float finalSample = outputSample * (1.0f - mixControl) + inputSample * mixControl;

            buffer.setSample(ch, i, finalSample);
        }

        // Advance indices
        fifoIndex++;
        outputBufferReadPos = (outputBufferReadPos + 1) % outputBufferSize;

        // 2. Check if we have enough data for a frame (FFT_SIZE)
        if (fifoIndex >= FFT_SIZE)
        {
            frameReady = true;
            // Reset index relative to the overlap for the next accumulation phase
            fifoIndex -= HOP_SIZE;
        }

        // Process the frame if ready
        if (frameReady)
        {
            for (int ch = 0; ch < numChannels; ++ch)
            {
                // Copy data from FIFO to processing buffer (channelTimeDomain)
                // The data corresponding to the current frame is at the start of the FIFO before shifting.
                std::copy(inputFIFO.getReadPointer(ch), inputFIFO.getReadPointer(ch) + FFT_SIZE, channelTimeDomain[ch].begin());

                processFrame(ch);
            }

            // Shift input FIFO by hopSize for all channels (Efficient FIFO management)
            for (int ch = 0; ch < numChannels; ++ch)
            {
                // Using std::rotate on the underlying float array
                float* data = inputFIFO.getWritePointer(ch);
                // This shifts the buffer content left by HOP_SIZE
                std::rotate(data, data + HOP_SIZE, data + FFT_SIZE);
            }

            // Advance output buffer write position for the next OLA
            outputBufferWritePos = (outputBufferWritePos + HOP_SIZE) % outputBufferSize;
        }
    }
}

// STFT core: FFT -> Modification -> IFFT -> OLA
void SpectralAnimatorEngine::processFrame(int channel)
{
    auto& timeDomain = channelTimeDomain[channel];
    auto& freqDomain = channelFreqDomain[channel];
    int outputBufferSize = outputBuffer.getNumSamples();

    // 1. Windowing (Analysis window)
    window.multiplyWithWindowingTable(timeDomain.data(), FFT_SIZE);

    // 2. Forward FFT
    // Copy time domain data to frequency domain buffer for FFT operation
    std::copy(timeDomain.begin(), timeDomain.end(), freqDomain.begin());
    forwardFFT.performRealOnlyForwardTransform(freqDomain.data());

    // 3. Spectral Modification
    const std::vector<float>& mask = (currentMode == Mode::Pitch) ? harmonicMask : formantMask;

    // FIX: Get the current morph value for this frame
    float currentMorph = smoothedMorph.getCurrentValue();


    // Iterate over bins (including DC and Nyquist)
    for (int k = 0; k < NUM_BINS; ++k)
    {
        float real, imag;
        // Unpack JUCE packed format (DC at [0], Nyquist at [1])
        if (k == 0) { real = freqDomain[0]; imag = 0.0f; } // DC
        else if (k == NUM_BINS - 1) { real = freqDomain[1]; imag = 0.0f; } // Nyquist
        else { real = freqDomain[2 * k]; imag = freqDomain[2 * k + 1]; }

        // Calculate Magnitude and Phase (Phase Vocoder core)
        float magnitude = std::sqrt(real * real + imag * imag);
        float phase = std::atan2(imag, real);

        // Apply Shaping Mask
        float modifiedMag = magnitude * mask[k];

        // Apply Morph Control (Linear interpolation)
        // FIX: Use the current smoothed value (currentMorph)
        float finalMag = magnitude * (1.0f - currentMorph) + modifiedMag * currentMorph;

        // 4. Convert back to Complex (using original phase)
        real = finalMag * std::cos(phase);
        imag = finalMag * std::sin(phase);

        // Pack back into JUCE format
        if (k == 0) { freqDomain[0] = real; }
        else if (k == NUM_BINS - 1) { freqDomain[1] = real; }
        else { freqDomain[2 * k] = real; freqDomain[2 * k + 1] = imag; }
    }

    // 5. Perform inverse FFT (In-place on freqDomain)
    // Note: JUCE FFT handles normalization internally.
    forwardFFT.performRealOnlyInverseTransform(freqDomain.data());

    // 6. Window (Synthesis window) and overlap-add
    // Copy result back to time domain buffer for synthesis windowing
    std::copy(freqDomain.begin(), freqDomain.begin() + FFT_SIZE, timeDomain.begin());
    window.multiplyWithWindowingTable(timeDomain.data(), FFT_SIZE);

    // Overlap-Add into the output buffer starting at the current write position
    for (int i = 0; i < FFT_SIZE; ++i)
    {
        int index = (outputBufferWritePos + i) % outputBufferSize;
        // Add the windowed frame to the accumulation buffer
        outputBuffer.addSample(channel, index, timeDomain[i]);
    }
}

// Mask generation for Pitch and Formant modes
void SpectralAnimatorEngine::updateMasks()
{
    if (sampleRate <= 0) return;
    float binWidth = (float)sampleRate / (float)FFT_SIZE;

    if (currentMode == Mode::Pitch)
    {
        // Pitch Mode: Create a harmonic mask using Gaussian peaks.
        std::fill(harmonicMask.begin(), harmonicMask.end(), 0.0f);
        float f0 = pitchHz;
        if (f0 < binWidth) return;

        // Define the width (sigma) of the harmonic peaks in bins
        const float harmonicWidth = 1.5f;
        const float widthSquared = harmonicWidth * harmonicWidth;

        for (int h = 1; ; ++h)
        {
            float freq = f0 * (float)h;
            if (freq >= sampleRate / 2.0f) break;

            float binIndex = freq / binWidth;
            int centerBin = (int)(binIndex + 0.5f);

            if (centerBin >= NUM_BINS) break;

            // Create the Gaussian peak
            // Iterate over a practical range around the center bin (e.g., +/- 3*sigma)
            int range = (int)(harmonicWidth * 3.0f);
            int startBin = juce::jmax(0, centerBin - range);
            int endBin = juce::jmin(NUM_BINS - 1, centerBin + range);

            for (int i = startBin; i <= endBin; ++i)
            {
                float distance = (float)i - binIndex;
                // Gaussian function: exp(-0.5 * (x/sigma)^2)
                float gain = std::exp(-0.5f * (distance * distance) / widthSquared);
                // Ensure we take the maximum if harmonics overlap
                harmonicMask[i] = juce::jmax(harmonicMask[i], gain);
            }
        }
    }
    else if (currentMode == Mode::Formant)
    {
        // Formant Mode: Use the interpolated vowel space
        FormantProfile vowel = getVowel(formantXY.x, formantXY.y);

        std::fill(formantMask.begin(), formantMask.end(), 0.0f);

        // Define formants (F1, F2, F3) and their bandwidths
        const std::array<float, 3> freqs = { vowel.f1, vowel.f2, 2500.0f }; // F3 fixed approximation
        const std::array<float, 3> bandwidths = { 100.0f, 150.0f, 200.0f }; // In Hz

        for (int f = 0; f < 3; ++f)
        {
            float centerFreq = freqs[f];
            float bw = bandwidths[f];

            // Create a resonant peak shape (using a Lorentzian/Cauchy model approximation)
            // Gain = 1 / (1 + ((freq - center) / bandwidth)^2)
            for (int k = 0; k < NUM_BINS; ++k)
            {
                float freq = k * binWidth;
                float normalizedDistance = (freq - centerFreq) / bw;
                float gain = 1.0f / (1.0f + normalizedDistance * normalizedDistance);
                // Take the maximum if formants overlap
                formantMask[k] = juce::jmax(formantMask[k], gain);
            }
        }

        // Normalize the mask so the maximum gain is 1.0 (preserves overall energy)
        float maxGain = *std::max_element(formantMask.begin(), formantMask.end());
        if (maxGain > 0.0f)
        {
            for (float& val : formantMask) val /= maxGain;
        }
    }
}

================================================================================
// File: SpectralAnimatorEngine.h
================================================================================

﻿//================================================================================
// File: FX_Modules/SpectralAnimatorEngine.h
//================================================================================
#pragma once
#include <juce_dsp/juce_dsp.h>
#include <vector>
#include <array>
#include <complex>
#include <cmath>
#include <algorithm>

class SpectralAnimatorEngine
{
public:
    static constexpr int FFT_ORDER = 11;
    static constexpr int FFT_SIZE = 1 << FFT_ORDER;
    static constexpr int HOP_SIZE = FFT_SIZE / 4;
    static constexpr int NUM_BINS = FFT_SIZE / 2 + 1;

    enum class Mode { Pitch, Formant };

    SpectralAnimatorEngine();
    void prepare(const juce::dsp::ProcessSpec& spec);
    void reset();
    void process(juce::AudioBuffer<float>& buffer);

    void setMode(Mode newMode);
    void setPitch(float newPitchHz);
    void setFormant(float x, float y);
    void setMorph(float amount);
    void setTransientPreservation(float amount);
private:
    void processFrame(int channel);
    void updateMasks();

    struct FormantProfile { float f1, f2; };
    FormantProfile getVowel(float x, float y);

    double sampleRate = 44100.0;
    int numChannels = 0;

    juce::dsp::FFT forwardFFT;
    juce::dsp::WindowingFunction<float> window;

    juce::AudioBuffer<float> inputFIFO;
    juce::AudioBuffer<float> outputBuffer;
    int fifoIndex = 0;
    int outputBufferWritePos = 0;
    int outputBufferReadPos = 0;

    std::vector<std::vector<float>> channelTimeDomain;
    std::vector<std::vector<float>> channelFreqDomain;

    struct TransientDetectorChannel {
        juce::dsp::FirstOrderTPTFilter<float> highPassFilter;
        juce::dsp::BallisticsFilter<float> envelopeFollower;
        float transientMix = 0.0f;
        float decayFactor = 0.99f;
    };
    std::vector<TransientDetectorChannel> transientDetectors;
    const float transientThreshold = 0.05f;

    // --- Parameters ---
    Mode currentMode = Mode::Pitch;
    float pitchHz = 440.0f;

    // ✅ FIX: Replaced juce::Point<float> with a simple internal struct
    struct XYPair { float x = 0.5f; float y = 0.5f; };
    XYPair formantXY;

    // FIX: Replaced raw floats with SmoothedValue for glitch-free modulation.
    // float morphAmount = 1.0f; // REMOVED
    // float transientPreservation = 1.0f; // REMOVED
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedMorph;
    juce::SmoothedValue<float, juce::ValueSmoothingTypes::Linear> smoothedTransientPreservation;


    // --- Spectral Masks (Shared across channels) ---
    std::vector<float> harmonicMask;
    std::vector<float> formantMask;
    bool masksNeedUpdate = true;
};


================================================================================
// File: SpectralAnimatorProcessor.cpp
================================================================================

//================================================================================
// File: FX_Modules/SpectralAnimatorProcessor.cpp
//================================================================================
#include "SpectralAnimatorProcessor.h"

SpectralAnimatorProcessor::SpectralAnimatorProcessor(juce::AudioProcessorValueTreeState& apvts, int slotIndex)
    : AudioProcessor(BusesProperties().withInput("Input", juce::AudioChannelSet::stereo(), true)
        .withOutput("Output", juce::AudioChannelSet::stereo(), true)),
    mainApvts(apvts)
{
    // Define Parameter IDs (Using a distinct prefix SPECANIM_)
    auto slotPrefix = "SLOT_" + juce::String(slotIndex + 1) + "_SPECANIM_";
    modeParamId = slotPrefix + "MODE";
    pitchParamId = slotPrefix + "PITCH";
    formantXParamId = slotPrefix + "FORMANT_X";
    formantYParamId = slotPrefix + "FORMANT_Y";
    morphParamId = slotPrefix + "MORPH";
    transientParamId = slotPrefix + "TRANSIENT_PRESERVE";
}

void SpectralAnimatorProcessor::prepareToPlay(double sampleRate, int samplesPerBlock)
{
    juce::dsp::ProcessSpec spec{ sampleRate, (juce::uint32)samplesPerBlock, (juce::uint32)getTotalNumInputChannels() };
    engine.prepare(spec);

    // Report the latency introduced by the STFT process (equal to the FFT Size).
    setLatencySamples(SpectralAnimatorEngine::FFT_SIZE);
}

void SpectralAnimatorProcessor::releaseResources()
{
    engine.reset();
}

void SpectralAnimatorProcessor::reset()
{
    engine.reset();
}

void SpectralAnimatorProcessor::processBlock(juce::AudioBuffer<float>& buffer, juce::MidiBuffer&)
{
    juce::ScopedNoDenormals noDenormals;
    // P1 Boilerplate
    auto totalNumInputChannels = getTotalNumInputChannels();
    auto totalNumOutputChannels = getTotalNumOutputChannels();
    for (auto i = totalNumInputChannels; i < totalNumOutputChannels; ++i)
        buffer.clear(i, 0, buffer.getNumSamples());

    // Update Parameters (Ensure parameters exist before accessing)
    if (!mainApvts.getRawParameterValue(modeParamId) || !mainApvts.getRawParameterValue(pitchParamId) ||
        !mainApvts.getRawParameterValue(formantXParamId) || !mainApvts.getRawParameterValue(formantYParamId) ||
        !mainApvts.getRawParameterValue(morphParamId) || !mainApvts.getRawParameterValue(transientParamId))
    {
        return;
    }

    // Load parameters and update the engine
    auto mode = static_cast<SpectralAnimatorEngine::Mode>(static_cast<int>(mainApvts.getRawParameterValue(modeParamId)->load()));
    float pitch = mainApvts.getRawParameterValue(pitchParamId)->load();
    float formantX = mainApvts.getRawParameterValue(formantXParamId)->load();
    float formantY = mainApvts.getRawParameterValue(formantYParamId)->load();
    float morph = mainApvts.getRawParameterValue(morphParamId)->load();
    float transient = mainApvts.getRawParameterValue(transientParamId)->load();

    engine.setMode(mode);
    engine.setPitch(pitch);
    engine.setFormant(formantX, formantY);
    engine.setMorph(morph);
    engine.setTransientPreservation(transient);

    // Process audio through the engine
    engine.process(buffer);
}

================================================================================
// File: SpectralAnimatorProcessor.h
================================================================================

//================================================================================
// File: FX_Modules/SpectralAnimatorProcessor.h
//================================================================================
#pragma once
#include <juce_audio_processors/juce_audio_processors.h>
#include "SpectralAnimatorEngine.h"

class SpectralAnimatorProcessor : public juce::AudioProcessor
{
public:
    SpectralAnimatorProcessor(juce::AudioProcessorValueTreeState& mainApvts, int slotIndex);
    ~SpectralAnimatorProcessor() override = default;

    const juce::String getName() const override { return "Spectral Animator"; }
    void prepareToPlay(double sampleRate, int samplesPerBlock) override;
    void releaseResources() override;
    void reset() override;
    void processBlock(juce::AudioBuffer<float>&, juce::MidiBuffer&) override;

    // Standard JUCE Boilerplate
    juce::AudioProcessorEditor* createEditor() override { return nullptr; }
    bool hasEditor() const override { return false; }
    bool acceptsMidi() const override { return false; }
    bool producesMidi() const override { return false; }
    // The STFT process introduces latency and a tail.
    double getTailLengthSeconds() const override { return 0.5; }
    int getNumPrograms() override { return 1; }
    int getCurrentProgram() override { return 0; }
    void setCurrentProgram(int) override {}
    const juce::String getProgramName(int) override { return {}; }
    void changeProgramName(int, const juce::String&) override {}
    void getStateInformation(juce::MemoryBlock&) override {}
    void setStateInformation(const void*, int) override {}

private:
    // The DSP engine instance
    SpectralAnimatorEngine engine;

    juce::AudioProcessorValueTreeState& mainApvts;
    // Parameter IDs
    juce::String modeParamId, pitchParamId, formantXParamId, formantYParamId, morphParamId, transientParamId;
};

================================================================================
// File: SpectralDiffuser.cpp
================================================================================

//================================================================================
// File: FX_Modules/SpectralDiffuser.cpp
//================================================================================
#include "SpectralDiffuser.h"

// JUCE 8 FIX (C2084): Remove the constructor implementation here.

void SpectralDiffuser::prepare(const juce::dsp::ProcessSpec& spec)
{
    int numChannels = (int)spec.numChannels;

    // Initialize FIFOs (size FFT_SIZE)
    inputFIFO.setSize(numChannels, FFT_SIZE);
    outputFIFO.setSize(numChannels, FFT_SIZE);

    // Initialize FFT data buffers (size 2*FFT_SIZE)
    fftData.resize(numChannels);
    for (auto& channelData : fftData)
    {
        channelData.resize(FFT_SIZE * 2);
    }

    reset();
}

void SpectralDiffuser::reset()
{
    fifoIndex = 0;
    inputFIFO.clear();
    outputFIFO.clear();
}

// FIX: Corrected the loop structure (Samples outer, Channels inner) for proper OLA implementation.
// The original implementation iterated Channel first, then Sample, breaking time alignment.
void SpectralDiffuser::process(juce::AudioBuffer<float>& buffer, float diffusionAmount)
{
    juce::ScopedNoDenormals noDenormals;

    int numSamples = buffer.getNumSamples();
    int numChannels = buffer.getNumChannels();

    // Bypass processing if diffusion is negligible
    if (diffusionAmount < 0.001f)
        return;

    // Iterate over samples first (Outer loop)
    for (int i = 0; i < numSamples; ++i)
    {
        // Iterate over channels for the current sample index 'i' (Inner loop)
        for (int channel = 0; channel < numChannels; ++channel)
        {
            float inputSample = buffer.getSample(channel, i);

            // 1. Push input sample into Input FIFO
            inputFIFO.setSample(channel, fifoIndex, inputSample);

            // 2. Retrieve output sample from Output FIFO (delayed by HOP_SIZE)
            float outputSample = outputFIFO.getSample(channel, fifoIndex);
            buffer.setSample(channel, i, outputSample);

            // Clear the sample we just read
            outputFIFO.setSample(channel, fifoIndex, 0.0f);
        }

        // 3. Advance FIFO index (Done after all channels are processed for this time step)
        fifoIndex++;

        // 4. Check if the input FIFO is full (ready for FFT)
        if (fifoIndex == FFT_SIZE)
        {
            // Process the frame for all channels
            for (int ch = 0; ch < numChannels; ++ch)
            {
                processFrame(ch, diffusionAmount);

                // Shift the input FIFO to handle the overlap
                // Move the second half (at index HOP_SIZE) to the beginning (at index 0).
                for (int j = 0; j < HOP_SIZE; ++j)
                {
                    inputFIFO.setSample(ch, j, inputFIFO.getSample(ch, j + HOP_SIZE));
                }
            }
            // Reset index to the start of the next frame accumulation
            fifoIndex = HOP_SIZE;
        }
    }
}

void SpectralDiffuser::processFrame(int channel, float diffusionAmount)
{
    auto& data = fftData[channel];

    // 1. Copy Input FIFO to FFT buffer
    std::copy(inputFIFO.getReadPointer(channel),
        inputFIFO.getReadPointer(channel) + FFT_SIZE,
        data.begin());

    // 2. Apply Analysis Window
    window.multiplyWithWindowingTable(data.data(), FFT_SIZE);

    // 3. Perform Forward FFT (in-place)
    fft.performRealOnlyForwardTransform(data.data());

    // 4. Process Spectrum (Phase Randomization)
    // FIX: Iterate from bin 1 up to (but not including) Nyquist (i < FFT_SIZE/2).
    // We MUST skip DC (i=0) and Nyquist (i=FFT_SIZE/2) as their phase must remain fixed for real signals.
    // The original code incorrectly iterated over and randomized these bins.
    for (int i = 1; i < FFT_SIZE / 2; ++i)
    {
        float real = data[2 * i];
        float imag = data[2 * i + 1];

        // Convert to polar coordinates
        float magnitude = std::sqrt(real * real + imag * imag);
        float phase = std::atan2(imag, real);

        // Add random phase shift scaled by diffusion amount
        phase += distribution(randomEngine) * diffusionAmount;

        // Convert back to rectangular coordinates
        data[2 * i] = magnitude * std::cos(phase);
        data[2 * i + 1] = magnitude * std::sin(phase);
    }
    // DC (data[0]) and Nyquist (data[1]) components remain untouched.


    // 5. Perform Inverse FFT (in-place)
    fft.performRealOnlyInverseTransform(data.data());

    // 6. Apply Synthesis Window
    window.multiplyWithWindowingTable(data.data(), FFT_SIZE);

    // 7. Overlap-Add into Output FIFO
    // We add the result starting at the beginning of the output FIFO.
    // For Hann window with 50% overlap, the gain is constant if the window normalization is correct (handled by JUCE).
    for (int i = 0; i < FFT_SIZE; ++i)
    {
        outputFIFO.addSample(channel, i, data[i]);
    }
}

================================================================================
// File: SpectralDiffuser.h
================================================================================

//================================================================================
// File: FX_Modules/SpectralDiffuser.h
//================================================================================
#pragma once
#include <juce_dsp/juce_dsp.h>
#include <vector>
#include <random>
#include <algorithm>
#include <cmath>

class SpectralDiffuser
{
public:
    // OPTIMIZATION: Reduced FFT Order from 11 (2048) to 10 (1024).
    // This significantly reduces CPU load while still providing effective diffusion.
    static constexpr int FFT_ORDER = 10;
    static constexpr int FFT_SIZE = 1 << FFT_ORDER;
    static constexpr int HOP_SIZE = FFT_SIZE / 2; // 50% Overlap

    // JUCE 8 FIX (C2039/C2065): Initialize using the WindowingMethod enum and a standard window.
    SpectralDiffuser()
        : fft(FFT_ORDER),
        window(FFT_SIZE, juce::dsp::WindowingFunction<float>::WindowingMethod::hann),
        distribution(-juce::MathConstants<float>::pi, juce::MathConstants<float>::pi)
    {
        // Ensure unique seeding
        randomEngine.seed(static_cast<unsigned long>(juce::Time::currentTimeMillis()));
    }


    void prepare(const juce::dsp::ProcessSpec& spec);
    void reset();
    void process(juce::AudioBuffer<float>& buffer, float diffusionAmount);
    int getLatencyInSamples() const { return HOP_SIZE; }

private:
    void processFrame(int channel, float diffusionAmount);

    juce::dsp::FFT fft;
    juce::dsp::WindowingFunction<float> window;

    juce::AudioBuffer<float> inputFIFO;
    juce::AudioBuffer<float> outputFIFO;
    std::vector<std::vector<float>> fftData;
    int fifoIndex = 0;

    std::minstd_rand randomEngine;
    std::uniform_real_distribution<float> distribution;
};

================================================================================
// File: TapeSaturation.h
================================================================================

﻿//================================================================================
// File: FX_Modules/TapeSaturation.h (CORRECTED V2)
// Description: Optimized polynomial tape saturation model.
//================================================================================
#pragma once
#include <juce_dsp/juce_dsp.h>
#include <vector>
#include <cmath>
#include <algorithm>

namespace TapeDSP
{
    /**
     * OptimizedTapeSaturator implementing cubic nonlinearity with asymmetry.
     * Optimized for high performance using SIMD.
     */
    class OptimizedTapeSaturator
    {
    private:
        // Define the filter type used for DC blocking
        using DCFilter = juce::dsp::IIR::Filter<float>;

    public:
        void prepare(const juce::dsp::ProcessSpec& spec)
        {
            numChannels = (int)spec.numChannels;

            // Initialize the std::vector of filters.
            dcBlockers.resize(numChannels);

            if (spec.sampleRate > 0)
            {
                // High-pass filter at 5Hz to remove offsets. Calculate coefficients once.
                auto coefficients = juce::dsp::IIR::Coefficients<float>::makeHighPass(spec.sampleRate, 5.0f);

                // Configure each filter instance.
                // We must use a MONO spec for individual IIR filters when managing them manually per channel.
                juce::dsp::ProcessSpec monoSpec = spec;
                monoSpec.numChannels = 1;

                for (auto& filter : dcBlockers)
                {
                    filter.prepare(monoSpec);
                    // Assign the coefficients.
                    *filter.coefficients = *coefficients;
                }
            }
            reset();
        }

        void reset()
        {
            // Reset all filters in the vector.
            for (auto& filter : dcBlockers)
            {
                filter.reset();
            }
            alpha = 0.0f;
            beta = 0.0f;
        }

        // Set the drive (0.0 to 1.0). Internally maps to the alpha coefficient.
        void setDrive(float drive)
        {
            // Alpha controls the intensity of the cubic term (x^3).
            // Constraining alpha to [0, 1/3].
            alpha = drive * 0.333f;
        }

        // Set asymmetry (0.0 to 1.0). Controls the quadratic term (x^2).
        void setAsymmetry(float asymmetry)
        {
            // Beta controls the amount of asymmetry (even harmonics).
            // Constraining beta for stability.
            beta = asymmetry * 0.2f;
        }

        // FIX: Complete the truncated file by adding the missing processSample and members.

        // Process a single sample.
        // Note: Requires the channel index 'ch' for accessing the correct DC blocker instance.
        float processSample(int ch, float input)
        {
            if (ch >= numChannels || ch < 0) return input;

            // 1. Apply DC blocking before saturation.
            // IIR::Filter::processSample takes one argument when prepared with mono spec.
            float x = dcBlockers[ch].processSample(input);

            // 2. Optimized polynomial saturation (Horner's method)
            // y = x * (1 + x * (beta - alpha*x))
            float y = x * (1.0f + x * (beta - alpha * x));

            // 3. Safety Clipping
            return juce::jlimit(-1.5f, 1.5f, y);
        }

    private:
        float alpha = 0.0f; // Controls cubic saturation (symmetric)
        float beta = 0.0f;  // Controls quadratic saturation (asymmetric)
        int numChannels = 0;

        // Vector of DC blocker filters (one per channel)
        std::vector<DCFilter> dcBlockers;
    };

} // namespace TapeDSP

